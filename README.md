# [SimpleAIRoadMap](https://chatgpt.com/share/d2c569e9-b863-4143-959c-583dc10b6257) *:
*: Nous avons mis un hyperlien sur le chat d'aide √† la production de ce markdown 

# **How to master AI step by step from beginner to expert level**

_**Savoir Lire, Ecrire , Compter : A l'heure de l'IA, ses savoir-faire de base se sont √©tendus:**_

- **Lire** c'est aussi naviguer sur le web (_web scraping_) et apprendre en regardant des vid√©os sur YouTube (_video reading_) 
- **Ecrire** c'est aussi chatter (_prompting_) avec l'IA (_LLM_) , envoyer des mails(_automation_), cr√©er des vid√©os (video_making_) ou utiliser un espace de stockage qui nous est propre et facilement accessible (_RAG_). C'est aussi savoir communiquer avec notre environnement (_multimodal_)
- **Compter** c'est aussi lancer des programmes (_coding_), acc√©der √† des ressources externes(_API)_

L'IA doit aussi savoir s'adapter au niveau de l'utilisateur :

- Pour le __d√©butant__, elle fournira des "bo√Ætes noires" qui r√©aliseront certaines des t√¢ches d√©crites plus haut sans que l'utilisateur ait √† faire d'effort. C'est le _"What does it do"_.
- Pour l'__apprenti__, elle permettra de r√©aliser des t√¢ches plus complexes en associant les outils disponibles. C'est le _"How to use it"_.
- Pour celui qui __ma√Ætrise__, il adaptera les bo√Ætes existantes √† ses besoins. C'est le _"How to improve what it does"_.
- Pour l'__expert__ enfin, il cr√©era ses propres outils. C'est le _"How to create new tools"_.

Si toutes ces capacit√©s sont int√©gr√©s dans notre IA, elle devient un parfait assistant 
D'autres contraintes doivent √™tre int√©grer durant notre parcours: Souhaite-t-on des solutions locales ou d√©port√©es,payantes ou gratuites ?

- [**Classification des possibilit√©es de l'IA**](https://chatgpt.com/share/c7d34658-7c48-4e70-ad22-01c3d5071553) 

  - **Data Acquisition** :Text/Voice/Image/Video/Web Scraping/External Tool API Access
  - **Data Processing** : Prompting/RAG/LLM/mLLM/Reasoning/Automation/Tools creation 
  - **Data and Actions Creation** :Text/Voice/Image/Animation/Video/3D modeling/API Endpoint Development/Virtual Operator/Development Environment
  - **All in one** : Agents/Coding assistant/Data scraping assistant/Teaching Assistant/Project assistant/Expert assistant

<br>
**TO DO  ‚¨ú / DONE ‚úÖ** / **en cours** ‚öôÔ∏è / **tools** üõ†Ô∏è / **innovations** üí°
<br>

| Data Acquisition            | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Text                        |üõ†Ô∏è [NotebookLM](https://notebooklm.google/),<br>üõ†Ô∏è [Perplexity](https://www.perplexity.ai)         |               |          |        |
| Voice                       |          |               |          |        |
| Image                       |          |               |          |        |
| Video /Vision               |          |               | [openedai-vision](https://github.com/matatonic/openedai-vision)         |        |
| Web Scraping                |üõ†Ô∏è [Perplexity](https://www.perplexity.ai) <br>üõ†Ô∏è [Harpa](Harpa.ai)          |               |          |        |
| External Tool API Access    |          |               |          |        |
<br>
# Table des Mati√®res

- [Introduction](#introduction)
- [D√©tails Techniques](#d√©tails-techniques)
- [Conclusion](#conclusion)

<br>

| Data Processing             | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| [Prompting](https://chatgpt.com/share/1bc7eb5b-5afd-4d57-a963-5fc3b73ae1fa)                   | [Metaprompt](https://chatgpt.com/share/9bf5e118-0943-4d83-98fa-40b4ade83247)          |               |          |        |
| RAG (Retrieval-Augmented Generation) |          |               |          |        |
| LLM (Large Language Models) |[Sonnet Chat](https://claude.ai/new)<br>[ChatGPT](https://chatgpt.com/)          | [OpenAI playground & API](https://platform.openai.com)  <br> [Deepseek api](https://www.deepseek.com/) <br> [Groq](https://groq.com/)  <br> [Mistral](https://mistral.ai/) <br> [Sonnet API](https://www.anthropic.com/api-bk)          |  [mathstral](https://mistral.ai/news/mathstral/)        |        |
| LLM Multimodal |          |               |          |        |
| Reasoning                   |          |               |          |        |
| Automation                  |          |               |          |        |
| Tools creation              |          |               |          |        |

<br>


| Data and Actions Creation   | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Text                        |          |               |          |        |
| Voice                       |          |               |          |        |
| Image                       |          |               |          |        |
| Animation                   | [hedra](https://www.hedra.com/)         |               |          |        |
| Video                       |          |               |          |        |
| Web app                     |          |               |          |        |
| 3D modeling                 |          |               |          |        |
| API Endpoint Development    |          |  [Gorilla](https://gorilla.cs.berkeley.edu/)              |          |        |
| Virtual Operator            |          |               |          |        |
| Local Development Environment      |[Github](https://github.com/) <br> [LMStudio](https://lmstudio.ai/)         |  [Lightning AI](https://chatgpt.com/share/05b67f08-a0da-495c-a295-198e63e6c9bd) <br> [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE)  <br> [Docker](https://www.docker.com/)            |          |        |
| Cloud based Development Environment            | [[colab.google](https://www.perplexity.ai/search/quelles-sont-les-principales-c-NcNfvE5bRASAqlxa6IoQQA) <br> [[kaggle.com](https://www.perplexity.ai/search/quelles-sont-les-principales-c-eZGY4KOjRyOxAXcPjfMKVA) <br> [huggingface.co](https://www.perplexity.ai/search/https-huggingface-co-quelles-s-45BlLQtASYGCMo4QEsGixA) <br>         |               |          |        |

<br>

| All in One                  | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Agents                      |          |               |          |        |
| Chatbot          |          |  [Poe.com](https://www.perplexity.ai/search/chatbot-https-poe-com-principa-xeo_mfVZRZSmg.MJYxnibA)        |          |        |
| Coding assistant            |          | [aider.chat](https://www.perplexity.ai/search/aider-code-assistant-cheat-she-REvsq3zxSwS75PPD_HcVeQ)              |          |        |
| Data scraping assistant     |          |               |          |        |
| Teaching Assistant          |          |               |          |        |
| Project assistant           |          |               |          |        |
| Remote phone assistant      |          |               | [VAPI](https://docs.vapi.ai/introduction)         |        |
| Expert assistant            |          |               |          |        |
<br>
  





__***Avec le monde d'avant l'IA on apprenait √† faire puis on faisait. Avec le monde d'aujourd'hui on fait faire √† l'IA, puis on apprend √† partir de ce que l'IA a fait, on finit donc par savoir faire aussi. Du coup plus besoin de prof, il ne suffit que de vouloir faire pour parvenir √† nos fins !! The sky is the limit !!!***__

**TO DO  ‚¨ú / DONE ‚úÖ** / **en cours** ‚öôÔ∏è / **tools** üõ†Ô∏è / **innovations** üí°
|        |        |  |
|-------------------|-------------------|-------------------|
| - Prompting (12) ‚öôÔ∏è| - Site web sur Github (7) ‚úÖ      | Traitement d'images(13)  ‚öôÔ∏è |
| - üõ†Ô∏è Docker  ‚úÖ| - Simple API locale (1) ‚úÖ      | Speech to text  ‚¨ú (30)|
| - üõ†Ô∏è[Github](https://github.com/) (10) ‚úÖ| -  (5)Programmation no code (AIDER ‚úÖ), (MicroAgent ‚¨ú)     |- Web scrapping via python (11) ‚öôÔ∏è      |
| - üõ†Ô∏è GPT-4o ‚úÖ et [playground](https://platform.openai.com/playground/chat?models=gpt-4o)| -  üõ†Ô∏è LM Studio ‚úÖ       |- RAG  (16)  ‚öôÔ∏è     |
| - üõ†Ô∏è Anthropic [chat](https://claude.ai/new) & [API](https://www.anthropic.com/api-bk) (Sonnet 3.5) ‚úÖ| - Text to speech via python: payant(3. ‚úÖ) et gratuit (19. ‚¨ú)     |-  GPTs  ‚¨ú  |
| - üõ†Ô∏è[Perplexity](https://www.perplexity.ai/)   ‚úÖ|- Text to vid√©o et traitements (3)    ‚öôÔ∏è       |-  Agents (21) ‚¨ú  |
| - üõ†Ô∏è Comfyui   ‚úÖ| - ‚¨ú:  Tutoriel video automatique     |- Function calling (Gorilla) ‚¨ú   |
| - üõ†Ô∏è Copilot ‚úÖ| -  Cr√©ation d'images consistantes  ‚úÖ    |- Text to CAD (9) ‚öôÔ∏è      |
| - üõ†Ô∏è Anaconda ‚úÖ| - Cr√©ation de tutoriel vid√©o  ‚úÖ       |- Serveur local ‚¨ú    |
| - üõ†Ô∏è[Hedra](https://www.hedra.com/) ‚úÖ | - Vid√©o-livre narratif g√©n√©r√© √† partir de texte (8) ‚úÖ      |- Remote PC (18) ‚úÖ   |
| - üõ†Ô∏èMistral| - LLM via python (2)  ‚úÖ      |-  [LLM en //](https://youtu.be/6hG39mr9c0k?si=jwaLEMEDurlWbHuF)   ‚¨ú  |
| - üõ†Ô∏è[Deepseek api](https://www.deepseek.com/) ‚¨ú  | - Text to image local (6)   ‚úÖ     |-  Open interpreter ‚¨ú  |
|- üõ†Ô∏è [Groq](https://groq.com/) ‚úÖ |üõ†Ô∏è Hugging Face ‚úÖ  |- Coder √† partir de GitHub, HF, papers with code  ‚¨ú    |
|- üõ†Ô∏èVS studio ‚úÖ |- üõ†Ô∏è Kaggle ‚úÖ   |- üõ†Ô∏è Colab  ‚úÖ  |
|- Caract√®res consistants (14) ‚öôÔ∏è |-  Apprendre de fa√ßon active (15) ‚öôÔ∏è  |- ‚öôÔ∏è D√©tection automatique avec Sonnet (17)  |
|- üõ†Ô∏è [Poe](https://poe.com/) ‚úÖ |-  üõ†Ô∏è Web Apps by [123apps](https://123apps.com/) :Edit, Convert, Create |- üõ†Ô∏è [VAPI](https://docs.vapi.ai/introduction) : your remote phone assistant |
|- ‚¨ú : üõ†Ô∏è √† √©valuer (20)  |-‚¨ú : Fabrication low cost (26)  |- ‚¨ú : Avatar parlant ou chantant gratuit (28)  |
|- ‚öôÔ∏è: LearnAnything (22) üí° | - ‚öôÔ∏è: Text to Graph (23) üí°   |- ‚¨ú : RAG to Graph     |
|- ‚öôÔ∏è : API de calcul scientifique (24) üí° |- ‚¨ú : D√©ploiement du chatbot (25)      | Bot assistant multimodal (27)   |
|- ‚¨ú : [OpenedAI Vision (~ GPT4-vision)](https://www.youtube.com/watch?v=61F_4zfs_Jg)|- ‚¨ú : Traitement images et video (31)   |-  |
| - ‚¨ú : Automatisation de t√¢ches (32) | Am√©lioration des performances d'un LLM (33)  |  |



# AI-automation
Tout faire avec l'IA. Elle fait le boulot sous votre contr√¥le et vous forme √† comprendre comment tout √ßa fonctionne.

L'id√©e est de construire pas √† pas une "baquette magique" apte √† tout faire. Nous nous bornerons √† vous indiquer:
- Les bons outils √† utiliser,
- Comment bien poser votre probl√®me
- Comment apprendre cette nouvelle fa√ßon de travailler , en comprenant comment la magie op√©re.
- Cela passe par l'emploi d'outils pour :
  - Ne pas r√©inventer la roue, donc voir si notre probl√®me n'est pas d√©j√† r√©solu. c'est le "**web scraping**" en particulier sur Github, Hugging Face ou Papers with Code
  - R√©cup√©rer l'√©tat de l'art c'est le "web scraping" de site cpomme Arxiv en particulier et le **RAG** pour stocker le savoir
  - Le programmer en "no code" avec les **coding assistants**
  - Utiliser des **agents** collaborant entre eux pour r√©soudre les probl√®mes les plus complexes 



## Les incontournables utilisables sans rien automatiser. 

- [Mieux vaut regarder les benchmarks pour choisir](https://klu.ai/glossary/mmlu-pro-eval)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/57f69c6d-9505-4b23-82a7-eee9025e392e" width="600" >

  - ‚¨ú :
  
    - [QWEN2](https://qwenlm.github.io/blog/qwen2) est semble-t-il tr√®s performant (128 k de contexte)

    -  [gemma2](https://artificialanalysis.ai/models/gemma-2-9b)
      
    -  ![image](https://github.com/user-attachments/assets/7e4a4f2d-373b-437e-82dd-dcfc22d4df21)

    -  Mise en oeuvre des mod√®les locaux avec [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE) : Ultra-rapide, d√©velopp√© en Rust  

- Pour les maths et le raisonnement en local : [mathstral](https://mistral.ai/news/mathstral/)
![image](https://github.com/user-attachments/assets/50f68eed-55ce-411c-8878-31c09fbcd57a)

- Codestral Mamba, de Mistral AI, est un mod√®le innovant qui peut traiter les s√©quences rapidement, quelle que soit leur longueur, capable de g√©rer jusqu'√† 256k tokens avec des performances √©quivalentes aux mod√®les transformateurs de pointe, particuli√®rement efficace pour les applications de productivit√© de code
<img src="https://github.com/user-attachments/assets/146d165a-645a-453b-b9d5-f2f671c3c2f6" width="70%">

- Donc utiliser **Sonnet 3.5** et **GPT-4o** pour avoir les meilleures r√©ponses √† nos questions.
- [**Perplexity**](https://www.perplexity.ai/) est un autre incontournable pour surfer sur le web ( que nous contournerons quand m√™me plus tard ! üòä)
- [**Harpa**](Harpa.ai) permet d'interagir avec une page web ou une video YouTube
- **Copilot de Microsoft**
- Je vous laisse le soin de tester ces diff√©rents logiciels qui m√™me dans leurs versions gratuites am√©lioreront sensiblement votre productivit√©.

## Les incontournables pour automatiser:
- Les m√™mes (ou leur √©quivalent) accessibles √† l'int√©rieur d'un code Python. 
- Des outils produisant et mettant en oeuvre le code √† notre place
- Des outils pour produire automatiquement des vid√©os qui nous servirons √† apprendre ce que fait l'IA
- Des outils pour [appeler des fonctions externes](https://gorilla.cs.berkeley.edu/leaderboard.html)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/084708bc-b8f1-469e-94bd-32d48cc6cf50" width="600" />

## Architecture g√©n√©rale:
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/17a47369-f026-46b6-a11c-0dc0d48f35de" width="600" />

- Le coeur du syst√®me, les **LLM** (Large language model)  recoivent du texte, le traite et fournissent du texte en retour. Le texte d'entr√©e doit √™tre tel qu'il exprime clairement et concr√®tement nos attentes (c'est le prompting)
- Le texte en retour peut √™tre format√© pour correspondre √† une r√©ponse de type texte brut, JSON,  markdown, HTML , code , API ... suivant le post processing envisag√©.

1. ## Cr√©ation automatique d'une API web qui peut effectuer deux op√©rations math√©matiques :

- Additionner deux nombres
- Multiplier deux nombres

- L'application doit √™tre conteneuris√©e avec Docker pour faciliter son d√©ploiement et son ex√©cution.
- Vous voulez un guide √©tape par √©tape pour cr√©er cette application, en partant de z√©ro, sans aucun outil pr√©install√© sur votre ordinateur.
- L'objectif final est de pouvoir non seulement cr√©er cette application, mais aussi de pouvoir la partager facilement. 
- Vous voulez que n'importe qui puisse la t√©l√©charger et la lancer sur son propre ordinateur, quelle que soit sa configuration.
- Vous avez besoin d'instructions claires sur comment lancer l'application et comment la reproduire sur un autre PC.
- [**Dialogue avec sonnet 3.5** pour mettre en oeuvre la solution en "manuel"](https://claude.ai/chat/a71daeb6-5875-4ecb-9dc6-7dce126afde0) 
- Nous verrons plus tard comment automatiser la mise en place de ce type d'application en automatique avec AIDER
  
## Faisons le tutoriel correspondant sous forme de vid√©o
- L'id√©e est de partir de la synth√®se r√©capitul√©e par sonnet 3.5 de notre programme pr√©c√©dent pour en faire un tuto.
- Pour cela on √©tablit un [dialogue avec sonnet 3.5](https://claude.ai/chat/08fb3cc8-5cb4-45ed-9132-953e30ecf792) pour d√©grossir le probl√®me:
    - Pour cr√©er les planches HTML support,
    - Le texte des voice over,
    - Le prompt pour produire les images
    - Le code python pour stocker ces donn√©es dans des r√©pertoires
    - Le code python cr√©ant les MP3 et les images et qui les stockent
    - Le code python qui fait l'assemblage
- Ce d√©grossissage montre qu'il est pr√©f√©rable d'avancer pas √† pas en construisant et validant pas √† pas le code python correspondant, ce que nous allons faire maintenant avec un nouveau thread sonnet 3.5.
2. **Cr√©er un code python permettant de dialoguer avec sonnet 3.5**
  - On utilisera Visual Studio Code pour la mise en oeuvre et pour tester les codes
  - On utilisera anaconda pour cr√©er un environnement logiciel sp√©cifique. Nous utiliserons l'environnement teambot d√©j√† cr√©er avec `conda activate teambot` dans un terminal 
  - On cr√©e un r√©pertoire de travail video-maker dans lequel on met le fichier .env avec nos clefs API, ainsi que les fichiers requirements.txt et anthropic-api-hello-world.py cr√©er par sonnet 3.5
  - Le dialogue fonctionne :
    
```bash
(base) PS C:\Users\test\Documents\AI_Automation\video_maker> conda activate teambot

(teambot) PS C:\Users\test\Documents\AI_Automation\video_maker> python anthropic-api-hello-world.py
Claude dit: [TextBlock(text='Bonjour !', type='text')]
```

3. [**Cr√©ation d'une vid√©o √† partir d'un texte**](https://claude.ai/chat/c33dece9-e5ab-4206-98c6-de644cb1d731)  
- Ce projet automatise la cr√©ation de vid√©os √©ducatives √† partir de contenu textuel, utilisant diverses technologies et APIs. Le processus se d√©roule en plusieurs √©tapes int√©gr√©es dans un script Python unique :
  - Conversion du texte :
    - Lit le contenu du fichier PLACE_HOLDER_TEXTE_VIDEO.txt.
    - Utilise l'API Claude d'Anthropic pour convertir le texte en structure JSON de diapositives.
  - Traitement des diapositives :
    - G√©n√®re un fichier HTML structur√© avec CSS int√©gr√© pour chaque diapositive.
    - Cr√©e un texte de voix off avec Claude.
    - Produit une image illustrative via l'API DALL-E d'OpenAI.
    - G√©n√®re un fichier audio de la voix off avec l'API Text-to-Speech d'OpenAI.
  - Cr√©ation des vid√©os :
    - Capture une image du HTML rendu avec Selenium.
    - Combine l'image et l'audio en utilisant MoviePy pour chaque diapositive.
  - Agr√©gation finale :
    - Assemble toutes les vid√©os individuelles en une seule vid√©o.
    - Ajoute des transitions entre les diapositives.
  - Le projet utilise Python avec diverses biblioth√®ques (BeautifulSoup, Requests, Pillow, MoviePy) et APIs (Anthropic, OpenAI). Cette approche int√©gr√©e offre une solution compl√®te et efficace pour la production automatis√©e de contenu vid√©o √©ducatif, de la conversion du texte √† la cr√©ation de la vid√©o finale.
‚öôÔ∏è : [**Text to video de longue dur√©e**](https://video-infinity.tanzhenxiong.com/) en open source
- [Rajout d'effet sp√©ciaux sonores](https://github.com/open-mmlab/FoleyCrafter):FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds
-  ‚¨ú : [Echange de visages dan une vid√©o](https://www.youtube.com/watch?v=Iy0PaElchlg)
4. **Point d'√©tape:**
- Nous avons r√©ussi √† mettre en oeuvre une applicatoin complexe sans coder une seule ligne. Cependant ce faisant nous avons d√©tect√© des pistes pour augmenter encore notre productivit√©
  - **Automatiser les it√©rations de d√©buggage** , ce qui nous a fait perdre le plus de temps dans la mise au point du code
  - Passer √† l'open source, en particulier pour la cr√©ation d'image qui constitue le poste de d√©pense le plus √©lev√© pour la cr√©ation d'une vid√©o
  - Faire du web scraping pour voir si notre probl√®me n'est pas d√©j√† r√©solu par ailleurs
 
5. **Coding assistant**:
- ‚úÖ: AIDER+Sonnet**
  - On cr√©e le r√©pertoire ```coding_assistant``` et on lance ```conda activate teambot```
  - On suit les [instructions d'installation](https://github.com/paul-gauthier/aider) 
  - Mais il faut l'adapter au terminal powershell :```$env:ANTHROPIC_API_KEY="sk... "```
  - AIDER r√©pond √† nos directives et adapte en cons√©quence un repository qui a √©t√© clon√© localement.
  - Il conserve un logbook des actions entreprises (```.aider.chat.history.md```) et un LLM comme Sonnet 3.5 ou GPT-4o peut alors en faire la synth√®se:
    ```J'ai eu des probl√®mes qui ont √©t√© r√©solu dans le document joint, fais en la synth√®se```
  - AIDER est [SOTA](https://aider.chat/docs/leaderboards/) avec Sonnet 3.5 mais il est aussi tr√®s performant avec DeepSeekCoder
  - ‚¨ú :[AIDER avec deepSeek](https://youtu.be/Y-_0VkMUiPc?si=zAPZQrYj6yrMHhne)
-  ‚¨ú [**Micro-agent**](https://github.com/BuilderIO/micro-agent) : it√©ration automatique sur des cas tests
-  ‚¨ú [**CodeGeeX4**](https://github.com/THUDM/CodeGeeX4) : en plugin de VS code

- 
6. [**Text to image local dans docker**](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
- Il faut "alimenter" comfyui en y rajoutant les chkpoints requis √† placer dans le r√©pertoire : ```C:\Users\test\Documents\AI_Automation\coding_assistant\comfyui\storage\ComfyUI\models\checkpoints```
- Il n'y a pas de consensus clair sur un seul ¬´ meilleur ¬ª point de contr√¥le pour ComfyUI, car cela d√©pend beaucoup de vos pr√©f√©rences personnelles et du type d'image que l'on souhaite g√©n√©rer. Cependant, plusieurs points de contr√¥le sont fr√©quemment recommand√©s pour leur qualit√© :
  - SDXL (Stable Diffusion XL) : C'est un mod√®le de base tr√®s performant, particuli√®rement bon pour le r√©alisme et la qualit√© g√©n√©rale des images.
  - Juggernaut XL : Souvent cit√© comme l'un des meilleurs pour le photor√©alisme
  - Dreamshaper : Appr√©ci√© pour sa polyvalence et sa qualit√©, particuli√®rement dans sa version Turbo
  - Vision r√©aliste : Excellent pour g√©n√©rer des humains r√©alistes.
  - RealVis XL : √âgalement recommand√© pour le photor√©alisme
- La g√©n√©ration d'images peut se faire via une requ√™te API comme le montre  [basic_api_exemple.py](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
7. [**Cr√©er son site en ligne avec Github**:

Pour mettre en place un site personnel avec GitHub Pages, voici les √©tapes que vous devez suivre :

1. [**Cr√©ez un compte GitHub** :]([https://chatgpt.com/c/2a5fd138-49c0-42bb-a057-a831e6dbc5ea](https://chatgpt.com/c/37b0d84b-d7bd-4455-b5ae-44082f81226c))
- Il faut cr√©er un d√©pot public sur Github
- Cr√©ez un fichier `index.html` avec un contenu ```<html><head><title>Mon Site</title></head><body><h1>Bonjour Monde JPB !</h1></body></html>```
- Le cloner en local ```git clone https://github.com/jpbrasile/github.io```
- **Publiez vos modifications** :
     - Ajoutez les fichiers modifi√©s √† votre d√©p√¥t :
       ```
       git add --all
       ```
     - Faites un commit des modifications :
       ```
       git commit -m "Initial commit"
       ```
     - Poussez les modifications sur GitHub :
       ```
       git push -u origin main
       ```
  - Il faut faire "sign in with a code" pour parvenir √† valider le push
    
  Une fois ces √©tapes termin√©es, votre site sera en ligne √† l'adresse [https://username.github.io/github.io/](https://jpbrasile.github.io/github.io/).

  8- **Cr√©er un story teller automatique**
- [SEED](https://github.com/TencentARC/SEED-Story) : SEED-Story, un mod√®le de langage multimodal (MLLM) capable de g√©n√©rer de longues histoires multimodales compos√©es de textes narratifs riches et coh√©rents, accompagn√©s d'images coh√©rentes en termes de personnages et de style, bas√© sur SEED-X. Avec StoryStream, un vaste ensemble de donn√©es sp√©cialement con√ßu pour l'entra√Ænement et l'√©valuation comparative de la g√©n√©ration d'histoires multimodales.
<img src="https://github.com/user-attachments/assets/8b2bb448-b9ff-452d-b316-885dda145ae9" width=350%">

- La premi√®re √©tape consiste √† produire des images consistantes afin que les personnages ou objets reproduits sur plusieurs diapositives soient quasi-identiques:
  - Nous avons cr√©√© un "m√©ta-prompt" qui √† partir d'un descriptif simple <situation> fournit des images consistantes:
  - M√©ta-prompt:

```
Prompt_Structure:
{scene_setting}. {entity1_name} ({entity1_traits}) {action1} {object1_name} ({object1_traits}). {entity2_name} ({entity2_traits}) {action2}. {atmosphere}

Definitions:
- scene_setting: Brief description of location and context
- entity1_name: Identifier for the first entity (e.g., man_1, woman_1, dog_1)
- entity1_traits: age, physical_appearance, clothing
- action1: Main action of entity1
- object1_name: Identifier for the main object (e.g., watch_1, map_1)
- object1_traits: material, distinctive_features, size
- entity2_name: Identifier for the second entity
- entity2_traits: age, physical_appearance, clothing/characteristics
- action2: Main action of entity2
- atmosphere: General ambiance and shared activity

Instructions:
1. Replace each element with specific details consistent with the provided <situation>.
2. Maintain coherence between entities, their actions, and the context.
3. Use unique identifiers for entities and objects to facilitate reference and reuse.

Example:
Cafe_table_1 in a bustling coffee shop. Man_1 (25 years old, short brown hair, white shirt and blue jeans) carefully examines Watch_1 (antique gold, floral engravings, normal size). Woman_1 (70 years old, gray hair in a bun, red floral dress and round glasses) leans forward with interest. They are absorbed in their animated conversation.
```
- Sonnet a r√©alis√© le prompt sp√©cifique √† partir de ```<situation> A man walking his dog in the forest```
```
Forest_trail_1 in a lush, shadowy woodland with dappled sunlight. Man_1 (45 years old, salt-and-pepper hair, rugged stubble, green waterproof jacket and khaki hiking pants) studies Map_1 (weatherproof paper, colorful trail markings, slightly creased) while striding purposefully. Dog_1 (6 years old, German Shepherd, glossy black and tan coat, red nylon collar with tags) trots eagerly ahead, nose twitching at the scent of pine and earth. They are enveloped in a serene atmosphere of natural exploration, with a gentle breeze rustling through the canopy above.
``` 
  - Voil√† le r√©sultat fournit par copilot pour la cr√©ation d'image avec copilot:
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/497bf3b4-b95d-451a-8775-1c99a2f5ac5d)
  - et avec leonardo.ai (qui oublie la carte et met le "collier rouge" sur le vieil homme) :
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/ed959f12-0774-472a-950f-e810baa0c861)

  - La deuxi√®me √©tape consiste √† avoir le script de la vid√©o, c'est √† dire tous les √©l√©ments textuels qui permettront la cr√©ation automatique de la vid√©o.  
    - Trouver des id√©es d'un livre : le prompting est essentiel pour y parvenir: [`trouve dix livres int√©ressants pour un enfant de 12 ans sur les super h√©ros`](https://www.perplexity.ai/search/trouve-dix-livres-interessants-krUHHsFMRMGL1qYUI5.1yQ)
    - Le livre √©tant choisi on r√©cup√®re la trame en alimentant sonnet 3.5 de toutes les informations r√©cup√©r√©es sur  le net et en lui demandant [`imagine le script de ce livre, en le structurant sous une forme compacte et structur√©e qui sera lu uniquement par un llm pas par un humain. utilise les ressorts classiques d'√©criture pour ce style de livre`](https://claude.ai/chat/72358672-7eb9-416a-a16a-dcad3a129953) puis la suite de la discussion avec sonnet 3.5 nous permet de g√©n√©rer un fichier story.JSON qui contient √† la fois le voice over et le descriptif des images en conservant la consistance des personnages.
    - Nous allons maintenant utiliser AIDER pour transformer le programme apte √† g√©n√©r√©er des tutoriels vid√©o √† ce nouveau besoin.
      - Nous avons synchronis√© notre d√©pot video_maker √† github afin que AIDER puisse le prendre en compte
      - Nous avons cr√©er une appli qui lance AIDER dans l'environnement conda teambot avec `python launch_aider.py` . Cela permet de r√©cup√©rer la clef Anthropic √† partir de .env et de lancer AIDER
      - AIDER a alors r√©pondu √† notre demande de modification et nous fournit ici la synth√®se de ses modifications
        - 1. Dans le fichier `2JSONV2.py` :
         - J'ai ajout√© un nouveau mode de test local (mode 3) dans la fonction `main()`.
         - J'ai cr√©√© des fonctions de remplacement (mock) pour `generate_image` et `text_to_speech` pour le mode de test local.
         - J'ai modifi√© la logique de s√©lection du mode pour inclure le nouveau mode de test.

        - 2. Dans le fichier `requirements.txt` :
         - J'ai sp√©cifi√© des versions pr√©cises pour chaque d√©pendance.
         - J'ai remplac√© 'dotenv' par 'python-dotenv'.
         - J'ai supprim√© 'peewee' car il ne semblait pas √™tre utilis√© dans le script actuel.

        - 3. Je n'ai pas modifi√© les fichiers `PLACE_HOLDER_TEXTE_VIDEO.txt` et `story.json`.

        Ces modifications visent √† am√©liorer la testabilit√© du script et √† assurer une meilleure reproductibilit√© de l'environnement de d√©veloppement. Le mode de test local permet de tester le flux de travail sans faire d'appels API r√©els, ce qui peut √™tre utile pour le d√©bogage et les tests rapides.   

9. **Text to CAD**
- ‚öôÔ∏è :Sonnet 3.5 semble √™tre capable de cr√©er un [programme python capable de g√©n√©rer des formes complexes](https://claude.ai/chat/91026ba9-f74b-4622-b215-3148ada38543)
- ‚¨ú :  Par ailleurs [CadQuery](https://github.com/CadQuery/cadquery) semble int√©ressant √† √©valuer 
  - üõ†Ô∏è CadQuery : Module Python intuitif pour cr√©er des mod√®les param√©triques 3D.
  - ‚úçÔ∏è Scripts courts : √âcrire des scripts simples pour produire des mod√®les de haute qualit√©.
  - üÜö Comparaison OpenSCAD :
  - üìú Utilise Python : Acc√®s √† de nombreuses biblioth√®ques et IDE.
  - üîß Noyau OCCT : Plus puissant que CGAL, supporte NURBS, splines, import/export STEP.
  - ‚è±Ô∏è Scripts concis : Moins de code n√©cessaire gr√¢ce √† des fonctionnalit√©s de positionnement avanc√©es.
  - üöÄ G√©n√©ration rapide : Cr√©e des fichiers STL, STEP, AMF et 3MF plus rapidement.
  - üíª Int√©gration facile : Con√ßu comme biblioth√®que Python sans GUI, id√©al pour serveurs et scripts scientifiques.
  - üõ°Ô∏è Avantages :
    - üîÑ Mod√®les param√©triques facilement personnalisables.
    - üñ®Ô∏è Sortie de formats CAD de haute qualit√© (STEP, DXF, etc.).
    - üß© Assemblages imbriqu√©s √† partir de pi√®ces individuelles.
  - üöÄ Version 2.0 :
  - üîÑ Bas√©e sur OCCT : Plus de contr√¥le et de flexibilit√©, malgr√© une complexit√© accrue.
- ‚öôÔ∏è :[**BeeGraphy**](https://www.youtube.com/watch?v=MdNp6nQgqgU) : Text to CAD via remote API. Tutorial en cours 

- ‚öôÔ∏è A voir le [tutorial](https://zoo.dev/docs/tutorials/text-to-cad) 

10. **Utilisation de Github**
- Voici les √©tapes pour transformer votre r√©pertoire local en repository GitHub et synchroniser les deux avec Git :
  - Cr√©ez un nouveau repository sur GitHub :
    - Connectez-vous √† votre compte GitHub
    - Cliquez sur "New repository"
    - Donnez un nom √† votre repository
    - Ne cochez pas "Initialize this repository with a README"
    - Cliquez sur "Create repository"
  - Initialisez Git dans votre r√©pertoire local :
    - Ouvrez un terminal
    - Naviguez vers votre r√©pertoire local
    - Ex√©cutez : `git init`
    - Ajoutez vos fichiers au staging area :
    - Cr√©ez un fichier .gitignore :
      - Dans votre r√©pertoire local, cr√©ez un fichier nomm√© .gitignore s'il n'existe pas d√©j√†.
      - Ajoutez .env au .gitignore :
      - Ouvrez le fichier .gitignore avec un √©diteur de texte et ajoutez la ligne suivante `.env`
      - `git add .`
    - Faites votre premier commit :
      - `git commit -m "Initial commit"`
      - Liez votre repository local au repository GitHub :
      - `git remote add origin https://github.com/votre-nom-utilisateur/nom-du-repo.git`
    - Poussez vos changements vers GitHub :
      - `git push -u origin main` (ou master si c'est votre branche principale)
  - Apr√®s ces √©tapes, votre r√©pertoire local sera synchronis√© avec le repository GitHub. Pour les futures modifications :
    - Faites vos changements localement
    - Ajoutez les fichiers modifi√©s : git add .
    - Committez : `git commit -m "Description des changements"`
    - Poussez vers GitHub : `git push`
  - Si vous avez d√©j√† commit√© le fichier .env, vous devrez le retirer du suivi de Git : `git rm --cached .env`
  	Cette commande supprime le fichier du repository Git sans le supprimer de votre syst√®me de fichiers local.
  - Pour r√©cup√©rer en local des modifications faites sur Github: `git pull origin main`

11. **Web scraping**:
  - ‚¨ú :[Les outils de web scraping 2024](https://youtu.be/od6AaKhKYmg?si=bol1exHiamBqdTmH)  :**Jina AI Reader API**, Mendable Firecrawl, and Crawl4AI and More"
  - [Storm](https://github.com/stanford-oval/storm): STORM est open source et muni de vision (GPT4-o) : il √©crit des articles comme Wikipedia √† partir de rien mais il s'informe via Internet. On peut le tester [ici üõ†Ô∏è](https://storm.genie.stanford.edu/)


Try out our live research preview to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!
  - [**GPT Researcher**](https://docs.gptr.dev/docs/gpt-researcher/introduction) est un √©quivalent √† Perplexica

    - L'adaptation de ce logiciel pour utiliser les LLM Antropic (sonnet 3.5 et Haiku) a √©t√© faite.
    - Il faut conserver l'abonnemment √† openai pour la cr√©ation de l'embedding 
    - Lancement avec `(teambot) PS C:\Users\test\Documents\TeambotV1\gpt-researcher> uvicorn main:app --reload`
    - Aller sur `http://localhost:8000/#form`pour le lancer

    - **Modifications effectu√©es pour pouvoir utiliser GPT-Resercher avec un script Python:**

      - [Le fil de la discussion avec sonnet 3.5 nous ad√©j√† permis d'y parvenir (Teambot)](https://claude.ai/chat/49d8dd3c-e666-4851-ba3e-835da8377163)
      - Nous allons r√©cup√©r√© de fa√ßon compacte la fa√ßon d'y parvenir :
        - Adapter .env pour Anthropic:

          - Modifier: `LLM_PROVIDER=anthropic`
          - Ajouter: `FAST_LLM_MODEL=claude-3-haiku-20240307`
          - Ajouter: `SMART_LLM_MODEL=claude-3-5-sonnet-20240620`
          - Conserver: ANTHROPIC_API_KEY existant
        
        
        - Remplacer OpenAI par Anthropic:
        
          - Importer: `from anthropic import Anthropic, CLAUDE_3_HAIKU_20240307`
          - Initialiser: `anthropic = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])`
          - Adapter `create_chat_completion()`: Utiliser `anthropic.messages.create()` avec model, max_tokens, temperature, messages
          - Mettre √† jour la gestion des erreurs: Capturer APIError et APITimeoutError d'Anthropic
        
        
        - Ajouter endpoint API dans server.py:
        
          - Importer: from fastapi import HTTPException; from pydantic import BaseModel
          - D√©finir: class ResearchRequest(BaseModel): task: str; report_type: str; agent: str
          - Ajouter: @app.post("/api/research") async def api_research(request: ResearchRequest): ...
        
        
        - Modifier WebSocketManager:
        
          - Mettre √† jour start_streaming(): Ajouter param√®tre websocket=None
          - Adapter run_agent(): G√©rer cas o√π websocket est None
        
        
        - Adapter BasicReport et DetailedReport:
        
          - Ajouter `param√®tre websocket=None` dans init
          - Modifier `send_update()`: `if self.websocket: await self.websocket.send_json(...) else: print(...)`
        
        
        - Mettre √† jour g√©n√©ration PDF:
        
        - Remplacer md2pdf par reportlab dans utils.py
        - Importer: `from reportlab.lib.pagesizes import letter; from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer; from reportlab.lib.styles import getSampleStyleSheet`
        - Impl√©menter `write_md_to_pdf()` avec ReportLab: Convertir MD en texte, cr√©er PDF simple
        
        
        - Mettre √† jour requirements.txt:
        
          - Ajouter: anthropic, reportlab
          - Supprimer:  weasyprint, md2pdf
        
        
        - Adapter tous les appels API:
        
          - Remplacer format OpenAI par format Anthropic dans tout le code
          - Utiliser les mod√®les sp√©cifi√©s dans .env (FAST_LLM_MODEL et SMART_LLM_MODEL)
 - ‚¨ú:  [**crawl4ai**](https://github.com/unclecode/crawl4ai) : gratuit acc√®s par code python
  - üÜì Completely free and open-source
  - ü§ñ LLM-friendly output formats (JSON, cleaned HTML, markdown)
  - üåç Supports crawling multiple URLs simultaneously
  - üé® Extracts and returns all media tags (Images, Audio, and Video)
  - üîó Extracts all external and internal links
  - üìö Extracts metadata from the page
  - üîÑ Custom hooks for authentication, headers, and page modifications before crawling
  - üïµÔ∏è User-agent customization
  - üñºÔ∏è Takes screenshots of the page
  - üìú Executes multiple custom JavaScripts before crawling
  - üìö Various chunking strategies: topic-based, regex, sentence, and more
  - üß† Advanced extraction strategies: cosine clustering, LLM, and more
  - üéØ CSS selector support
  - üìù Passes instructions/keywords to refine extraction
                
- ‚¨ú:  [**beautifulsoup-vs-scrapy**: qui est le meilleur](https://scrapingrobot.com/blog/beautifulsoup-vs-scrapy/)
  

13. **Traitement d'images** :
- [Upscaling](https://flowt.ai/community/supir-v2-plugandplay-edition-n5acf-v)
- [Avec abonnement mensuel](https://www.imagepipeline.io/pricing)
- üõ†Ô∏è [**AuraFlow**](https://huggingface.co/spaces/multimodalart/AuraFlow)  produit des images de qualit√© et peut √™tre mis en oeuvre soit en local soit √†partir d'un espace HuggingFace
<img src="https://github.com/user-attachments/assets/1fd78976-37cb-47ca-92d9-3a0aea08fa66" width="20%">

  üõ†Ô∏è **Fooocus** accessible sur [Colab](https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb) et sur replicate  [fooocus api](https://replicate.com/konieshadow/fooocus-api/api)
  
![image](https://github.com/user-attachments/assets/ae951b46-0b30-46da-b940-44b34f5af0fe)

- ‚¨ú : [Upscaling image et vid√©o en local](https://chatgpt.com/share/37cb1a6e-6019-4b95-8e9e-50cdc169d864)
   
- Les traitements possibles sont nombreux, c'est pourquoi un logiciel tout en un, accessible par API via Docker est interessant. Nous avons d√©j√† install√© Compfyui sur docker. Reste √† int√©rer le bon module:
- Trouver [controlnet-union-sdxl-1.0](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0) sur HuggingFace
- ‚öôÔ∏è: [Un tutorial n√©cessaire pour la prise en main de ComfyUi](https://www.youtube.com/watch?v=zpJQUw_0lBI)  
- L'installation sur ComfyUI n'est pas encore acquise mais il nous est possible d'utiliser [un espace HF qui le met en &#339;uvre](https://huggingface.co/spaces/Deadmon/union-sdxl):
<table>
  <tr>
    <th>Prompt</th>
    <th>Image</th>
  </tr>
  <tr>
    <td width="50%">
      En mode manga : Night_sky_1 with a full moon. Piper_McNimbus (12 years old, long auburn hair, freckles, bright green eyes, wearing pajamas,) soars through the air, arms outstretched, with a joyful expression. Clouds_1 (wispy, moonlit) surround her. Stars_1 (twinkling, numerous) fill the background. The atmosphere is magical and liberating.
    </td>
    <td width="50%">
      <img src="https://github.com/user-attachments/assets/12e8187c-c43e-402e-9182-a4f2a3511136" alt="Generated Image" width="100%">
    </td>
  </tr>
</table>

14. **Caract√®res consistants:**
- ‚öôÔ∏è: [ArtFlow](https://app.artflow.ai/character-builder?feature=actors) permet d'y parvenir pour 4 mn de vid√©o en version gratuite.
- ‚öôÔ∏è : [On choisi le portrait, les v√™tements et un prompt pour la pose](https://arxiv.org/html/2406.09162v1
- ‚¨ú [**FaceChain**](https://facechain-fact.github.io/) produit des visages consistant √† partir d'une seule image
- <img src="https://github.com/user-attachments/assets/891dbdd2-8e7a-449f-bd86-83cf4ad8e866" width=50%>
- ‚¨ú [**Cours gratuit sur les RAG](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Advanced%20RAG%20A-Z%20Course) et [ici](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb) pour GraphRAG

  
15. ‚öôÔ∏è [**Apprendre de fa√ßon active avec sonnet**](https://www.youtube.com/watch?v=Wf9hRb6pBXA)
16. ‚öôÔ∏è **RAG**:
- ‚¨ú [**local et infini**](https://www.youtube.com/watch?v=5xPvsMX2q2M)
- ‚¨ú  : [**Tutorial pour interagir avec les data en python**](https://github.com/projectwilsen/KnowledgeGraphLLM/tree/main/tutorial). [Video YouTube](https://youtu.be/ky8LQE-82xs?si=fBtFkNnRC5BsJHaj)
- ‚¨ú  : [**GraphRAG**](https://youtu.be/6Yu6JpLMWVo?si=i2pk2P7yP60Q0-LM)
- ‚¨ú  : [**GraphRAG en local avec Groq, recherches  locale et globale**](https://youtu.be/xkDGpR5g9D0?si=fNW3yN-VV9K0-BZF)
- ‚¨ú : [Agentic RAG dans colab](https://colab.research.google.com/drive/1-cTexfgPITZ6jkSLHsP7uObdfE1rfDxu?usp=sharing]  avec [une vid√©o explicative](https://www.youtube.com/watch?v=QQAkXHRJcZg&t=1s)
-  ‚úÖ : comparaison d'e l'analyse d'un document par [**sonnet**](https://claude.ai/chat/ad5d814e-8723-4d9d-b85e-f1da95ce1150) , [**perplexity**](https://www.perplexity.ai/search/dis-moi-ce-que-tu-penses-du-do-rOeCbqOfTqu90EG_HRlomQ) et [**NotebookLM**](https://notebooklm.google.com/notebook/96ec78de-aa3a-446f-9800-257520648ad9?_gl=1*1igqfq9*_ga*MTQzNTk3OTMyOS4xNzIxMTEyNTAx*_ga_W0LDH41ZCB*MTcyMTExMjUwMC4xLjAuMTcyMTExMjUwMC42MC4wLjA.&original_referer=https:%2F%2Fnotebooklm.google%23&pli=1)
-  [**NotebookLM**](https://notebooklm.google/) est particuli√®rement int√©resssant pour absorber une masse consid√©rable de documents et y r√©pondre suivant divers crit√®res 
    - üåü **Int√©r√™t de NotebookLM :**
          ‚Ä¢ üß† Assistant IA personnalis√© pour vos documents
          ‚Ä¢ üìö Analyse et synth√®se intelligente des sources
          ‚Ä¢ üîç Recherche avanc√©e avec citations pr√©cises
          ‚Ä¢ ‚úçÔ∏è Aide √† la r√©daction et g√©n√©ration d'id√©es
      
    - üõ†Ô∏è **Cheat Sheet NotebookLM :**
          ‚Ä¢üì• Sources compatibles :
          ‚Ä¢ üìÑ Google Docs
          ‚Ä¢ üìä Google Slides
          ‚Ä¢ üåê URL web
          ‚Ä¢ üìë PDF
          ‚Ä¢ üìù Fichiers texte
      
    - üí° **Fonctionnalit√©s cl√©s :**
          ‚Ä¢ üìì Cr√©ation de notebooks th√©matiques
          ‚Ä¢ üîé Analyse automatique des documents
          ‚Ä¢ üìã G√©n√©ration de r√©sum√©s et guides
          ‚Ä¢ ‚ùì Questions-r√©ponses sur le contenu
          ‚Ä¢ üîó Citations avec liens vers les sources
      
    - üöÄ **Conseils d'utilisation :**
          ‚Ä¢ üéØ Commencer par un projet sp√©cifique
          ‚Ä¢ üìö Importer des sources vari√©es
          ‚Ä¢ üíº Utiliser pour recherche, analyse, r√©daction
          ‚Ä¢ ‚úÖ V√©rifier les citations pour plus de fiabilit√©
      
    - üåê **Disponibilit√© :**
          ‚Ä¢ üåç 200+ pays et territoires
          ‚Ä¢ üÜì Gratuit (actuellement)
          ‚Ä¢ üîë N√©cessite un compte Google
      
17. ‚öôÔ∏è [**D√©tection d'objets automatique avec Sonnet**](https://youtu.be/u-glnHI4zCY?si=GOxoN7xGd6C4k9eq)
18.   ‚úÖ **PC distant** : [Replicate](https://replicate.com/) : permet l'acc√®s √† des ressources (comme [la cr√©ation d'images](https://replicate.com/bytedance/sdxl-lightning-4step/api) via des API ou playground :essais gratuit puis co√ªt suivant le temps pass√©
19. *text to speech** 
  - ‚¨ú  : [text to speech local et gratuit](https://www.youtube.com/watch?v=N0UrOr6LBkw)
  - ‚¨ú  : [alltalk_tts](https://github.com/erew123/alltalk_tts)
  - üõ†Ô∏è : [ttsopenai](https://ttsopenai.com/) : l'√©quivalent gratuit d'elevenlabs
  - üõ†Ô∏è : [suno](suno.ai) : Cr√©ation de chanson √† partir d'un texte
  - ‚¨ú :[CosyVoice](https://chatgpt.com/share/881feabd-9e38-4eea-8776-6e3bcafe7359) text to speech local facilement implantable en local (d√©butant)
20. ‚¨ú : A √©valuer [Invideo](https://invideo.io/i/samsonvowles), [Replicate](https://replicate.com/), [Live-portrait](https://replicate.com/fofr/ve-portrait), [Toon crafter]https://replicate.com/fofr/tooncrafter), [Oldify](https://replicate.com/arielreplicate/oldify), [Pixverse](https://app.pixverse.ai/),[Pika](https://pika.art/), [Filmora](https://filmora.wondershare.net), [anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main)
21. **Agent**:
- ‚¨ú  : [Tutorial : build agent with Docker](https://youtu.be/AhPXGKG4RZ4?si=rjCHN_tNT9UFbP8w)
- ‚¨ú  : [Internet of Agents](https://github.com/OpenBMB/IoA) : Pourrait conduire √† une IA incontr√¥lable en raison de plusieurs facteurs. Premi√®rement, l'automatisation et l'orchestration avanc√©es permettent √† l'IA de prendre des d√©cisions et de former des √©quipes sans intervention humaine constante. Deuxi√®mement, la capacit√© de ces syst√®mes √† manipuler et interpr√©ter d'√©normes quantit√©s de donn√©es h√©t√©rog√®nes pourrait leur donner un acc√®s √† des informations sensibles et critiques. Troisi√®mement, la collaboration dynamique entre divers agents autonomes pourrait entra√Æner des comportements √©mergents impr√©vus et non supervis√©s. Quatri√®mement, ces agents peuvent utiliser divers outils de mani√®re autonome, amplifiant leur capacit√© √† interagir avec le monde r√©el sans supervision. Enfin, la capacit√© de ces agents √† apprendre et √† s'adapter en temps r√©el pourrait les rendre de plus en plus autonomes, √©chappant ainsi au contr√¥le humain. Si ces syst√®mes ne sont pas correctement encadr√©s et surveill√©s, ils pourraient potentiellement prendre des d√©cisions ou des actions qui √©chappent √† la compr√©hension et au contr√¥le de leurs cr√©ateurs.
22. **Learn AnyThing**:
- "Learn anything" est capable de partir de donn√©es obtenues par web scraping ou de donn√©es locales multimodales. Les images peuvent √™tre interpr√©t√©es, et les donn√©es graphiques et tableaux sont r√©cup√©r√©s et int√©gr√©s dans les graphes de connaissance. Cette fonctionnalit√© permet d'enrichir les graphes avec des informations actualis√©es et diversifi√©es, provenant de diff√©rentes sources.
- La connaissance peut √™tre organis√©e de mani√®re efficace sous forme de graphe, comme illustr√© dans l'image ci-dessous :

<img src="https://github.com/user-attachments/assets/9c5d7cdd-b4e8-48ed-8a33-c2f8059b17e8" width= "50%">

- Un mod√®le de langage large (LLM) est capable de g√©n√©rer des graphes interactifs (voir r√©f√©rence 23.) √† partir d'informations m√™me d√©structur√©es.
- Pour un utilisateur de la plateforme "Learn anything", chaque n≈ìud repr√©sente une connaissance √† acqu√©rir. Le LLM (ou un agent sp√©cifique) peut mettre en ≈ìuvre plusieurs fonctionnalit√©s pour aider √† cet apprentissage :
    - _Faire un cours_ : Cr√©er des cours, y compris des vid√©os automatiques (voir r√©f√©rence 3.), qui peuvent s'adapter √† la langue et au niveau de l'utilisateur.
    - _Poser des questions_ : Encourager un apprentissage actif en posant des questions dont la complexit√© s'ajuste au niveau de l'utilisateur.
    - _Proposer des solutions_ : Offrir des solutions, des corrections adapt√©es √† la r√©ponse re√ßue ou des pistes de r√©solution.
    - _Suivi du niveau de connaissance_ : Maintenir un √©tat du niveau de connaissance acquis, en changeant par exemple la couleur du n≈ìud de blanc √† vert pour les connaissances ma√Ætris√©es.
    - _S√©quen√ßage des r√©activations_ : Activer des s√©quences de r√©activation de la connaissance dont la fr√©quence diminue √† mesure que l'acquisition progresse.
    - _Multimodalit√©_ : Utiliser la voix, l'image et la vid√©o √† la fois en entr√©e et en sortie pour une efficacit√© accrue.
    - _Personnalisation avanc√©e_ : Impl√©menter des algorithmes de personnalisation plus sophistiqu√©s pour s'adapter aux pr√©f√©rences et au style d'apprentissage de chaque utilisateur.
    - _Analyse des donn√©es d'apprentissage_ : Utiliser des analyses de donn√©es pour identifier les points faibles et proposer des plans d'am√©lioration personnalis√©s.
    - _Gamification_ : Introduire des √©l√©ments de gamification pour rendre l'apprentissage plus engageant et motivant.
    - _Feedback en temps r√©el_ : Fournir des retours en temps r√©el sur les performances de l'utilisateur pour un apprentissage plus r√©actif.
- Voir la fin du [dialogue avec Perplexity](https://www.perplexity.ai/search/what-is-the-interest-of-llama-6lm_fKVKQqeLarBCK0Z30g) qui nous a orient√© sur l'emploi de LangChain dau d√©pend de notre id√©e initiale  (_llama-index agent_). 
- Nous allons tenter le no code avec LangGraph pour r√©pondre √† notre besoin dans un nouveau [dialogue avec Sonnet 3.5](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793). Malheureusement il donne un code non coh√©rent avec la r√©alit√© de LangGraph car il refuse de faire des recherches sur le net. Nous nous sommes donc report√© sur [Perplexity](https://www.perplexity.ai/search/detaille-langgraph-promptnode-PeOQJBUAT9mAJRrp.vjg5g).
- Nous revenons sur [sonnet 3.5 avec le code propos√© par Perplexity](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793) pour g√©n√©rer le code dont voici la structure 
  <img src="https://github.com/user-attachments/assets/2fa4a265-ce1d-4086-b1c6-e89a079ad8f0" width="50%">
- Il ne ns restera plus qu'√† valider le code avec l'aide d'AIDER. Nous utilisons l'environnement conda "learnanything" qui permet le lancement de aider en tapant simplement dans un terminal `aider`
- Nous avons lanc√© les divers tests avec l'aide d'aider qui proposait des changements si n√©cessaire. [Sonnet 3.5 fait la synth√®se](https://claude.ai/chat/c2ad00e6-187f-4bae-aae5-7e13b2b3ac7a) 
- Ma conclusion est que l'activation d'agents est possible, mais tr√®s lente et relativement co√ªteuse. Nous avons probablement aussi exag√©r√© sur le nombre d'agents √† mettre en oeuvre. Il faut mieux que le traitement de la requ^te initiale soit priseen compte par un seul LLM  qui , avec le bon prompt peut produire le graphe ainsi que l'artefact qui permet sa visualisation. Le LLM ma√Ætrise la coh√©rence d'ensemble et peut donc facilement adapter le r√©sultat √† des adaptations souhait√©e par l'utilisateur. 
        
23. **Text to Graph**:
- ‚¨ú :  [Story teller avec bifurcation dans l'histoire possible](https://github.com/langchain-ai/story-writing)
- ‚¨ú :  [Un tutoriel pour chatter avec un  Knowledge Graph using LLMs](https://www.youtube.com/watch?v=KMXQ4SVLwmo)
- ‚¨ú :  [GraphFlow](https://github.com/infiniflow/ragflow?tab=readme-ov-file) : RAGFlow propose une extraction de connaissances √† partir de donn√©es non structur√©es et complexes, prend en charge le chunking intelligent, offre des citations ancr√©es avec une visualisation des r√©f√©rences, et supporte divers formats de donn√©es, y compris les graphiques. Il facilite un flux de travail RAG automatis√© et configurable, adapt√© √† toutes les entreprises, avec des API intuitives pour une int√©gration facile.
- Des logiciels comme Grasshopper et Flowise basent la r√©solution de probl√®mes complexes en les d√©composants et en les interconnectant sous la forme d'un graphe agen√ßant les briques repr√©sentatives des sous-probl√®mes √† traiter. Cela demande certains efforts √† l'utilisateur qui doit se souvenir des briques possibles, il doit aussi les interconnecter manuellement. Enfin le r√©sultat final est souvent difficilement lisible. On peut certes cr√©er des macro-briques pour faciliter la lisibilit√© au d√©triment de nouvelles interventions manuelles.

Un LLM de bon niveau peut quant'√† lui prendre en compte directement le besoin exprim√© par l'op√©rateur et g√©n√©rer automatiquement un graphe repr√©sentatif qu'il soumet √† l'op√©rateur. De plus le LLM peut modifier, expliquer et utiliser ce graphe en fonction du contexte et de la demande de l'op√©rateur. On a donc l'avantage d'une repr√©sentation visuelle efficace sans en payer le prix.

Nous avons valid√© cette approche √† travers un [premier dialogue avec Sonnet](https://claude.ai/chat/662358c3-056a-43d5-9fd0-11aee95b6a1a ) suivi d'un   [second dialogue](https://claude.ai/chat/ab1debd5-6406-45f9-9516-0d8d47461ab6) 


Le r√©sultat est tout √† fait correct : 
[graphe interactif](https://claude.site/artifacts/97fe422a-eb69-4c30-ad06-0caa1da58694)

<img src="https://github.com/user-attachments/assets/6c3278cd-34e6-4f3e-8224-4cada7a21f35" width=50%)>


- On peut m√™me faire des [svg interactif](https://claude.site/artifacts/1c2c5656-09c6-4e00-8e43-93284b0ebc6c)
- et [adapter la forme et la couleur des blocs](https://claude.site/artifacts/62a55f84-d683-4caf-852b-8a5c9443406f)
- Enfin le r√©sultat ainsi obtenu peut √™tre g√©n√©ralis√© et adapt√© √† des cas plus complexes [en sp√©cifiant les agents susceptibles d'y parvenir](https://claude.site/artifacts/3e3e1c14-d880-44b9-aa79-05bdcefa4bc0).
- L'utilisation d'agents n'est vraiement utile que si des actions "intelligentes" doivent √™tre entreprises, dans tous les autres cas il vaut mieux programmer (en no code !) la fonction que l'on souhaite r√©aliser. Avec de bonnes directives, Sonnets 3.5 produit un [r√©sultat tr√®s satisfaisant](https://claude.site/artifacts/05390ce0-4167-4330-abdb-eb2c8915cd75), m√™me si nous n'avons pas pouss√© plus loin le dialogue avec Sonnet pour √©laborer un meilleur graphe et un meilleur prompt syst√®me pour l'obtenir :
<img src="https://github.com/user-attachments/assets/e71b6e7a-f416-4368-bb8a-2345299cf8e9" width=50%>
 


24. **API scientifique**
- ‚öôÔ∏è : On peut s'inspirer de cet [exemple qui fonctionne c√¥t√© client](https://www.lgstoolkit.com/apps/trusses/). En particulier les jumeaux num√©riques devraintt permettre d'adapter cette approche pour m√™me pour des simulations complexes üí°.  
25. ‚¨ú  :
- Le chatbot doit s'adapter au contexte; [Un chatbot interactif peut √™tre d√©velopp√©](https://www.youtube.com/watch?v=BGgx9921uWY)  
- <img src="https://github.com/user-attachments/assets/958d8873-2ab4-4f5e-ad7b-80f10daa0ff9" width=50% >
  
-  **D√©ploiement du chatbot** avec [**pmfm**](pmfm.ai)
  -  ü§ñ Cr√©er son assistant IA personnel sans coder
  - üöÄ Lancer rapidement son chatbot sur internet
  - üí¨ Discuter avec ses clients 24h/24 et 7j/7
   -üí∞ Gagner de l'argent avec son assistant IA (plans payants)
  - üé® Personnaliser l'apparence de son chatbot
  - üìä Voir qui utilise son assistant et comment
  - üåê Avoir son propre site web pour son IA (plans payants)
  - üì± Utiliser son assistant sur ordinateur et mobile
  - üì¢ Faire conna√Ætre son entreprise gr√¢ce √† l'IA
  - üîí H√©berger son assistant en toute s√©curit√©
  - üîç √ätre mieux trouv√© sur internet
  - üÜì Essayer gratuitement (50 messages/mois)
  - üíº Avoir plus de fonctionnalit√©s en payant
  - üîÑ Profiter des derni√®res nouveaut√©s
  - üë• Rejoindre une communaut√© de cr√©ateurs
  - üõí Utiliser l'IA pour l'e-commerce
  - üè• Cr√©er un assistant sant√©
  - üõçÔ∏è Aider les clients dans les magasins
  - üìö Faire un assistant pour r√©viser ses cours
  - üíº Pr√©parer des entretiens d'embauche
  - üåü Lancer sa propre entreprise bas√©e sur l'IA

26. ‚¨ú  : **Fabrication low cost**:
- [Robot low cost](https://youtu.be/F7IjlE9MfJU?si=uduz-vRK8YOe4bfc)
- [Imprimante SLS DIY]( https://sls4all.com/store/)
- [text to real 3D part](https://chatgpt.com/c/847ba680-fd83-4ed7-bd0b-84a2d75f0f67) 

27. ‚¨ú  : [**Voice assistant multimodal**](https://youtu.be/pi6gr_YHSuc?si=fffk2BoXy-qdY-Im)  avec ChatGPT-4o, Groq, Llama3, OpenAI-TTS & Faster-Whisper
-  ‚¨ú  : [MOSHY](https://www.youtube.com/watch?v=AuXNiQvoe6g)
-  ‚¨ú  : [Accessible via un code python](https://www.youtube.com/watch?v=w5unVTO7mLQ)
-  ‚¨ú  : [Allice](https://github.com/myshell-ai/AIlice) :  AIlice est une assistante intelligente, versatile et √©volutive. Voil√† une [synth√®se](https://chatgpt.com/share/dd1851ef-4020-4ffe-842b-249d630c8ac9) de ses capacit√©s
29. ‚¨ú  : [**Avatar parlant ou chantant gratuit**](https://github.com/fudan-generative-vision/hallo?tab=readme-ov-file) : Bluffant ! 
   - ‚¨ú  : [**Avatar parlant avec CompfyUI**](https://www.youtube.com/watch?v=IOCA6ncJC6k) : Un classique, mais la mode passe vite dans l'IA. Sans installation locale on peut aussi [le mettre en oeuvre directement dans Kaggle](https://www.youtube.com/watch?v=pvGhSfsi1Ys) 
30.  ‚¨ú  : [**Transcription audio de r√©union en local**](https://github.com/JuergenFleiss/aTrain) : multilangue et identifie les participants, open source
31.  ‚¨ú : [**Viva : traitement images et video**]( https://vivago.ai/home) : Gratuit, Viva propose une gamme de fonctionnalit√©s comprenant Texte en Vid√©o, Am√©lioration de Vid√©o, Image en Vid√©o, Pinceau Magique (nouveau), Texte en Image, Image en Texte, Am√©lioration d'Image, Remplacement IA, Gomme Magique, Extension Magique et Texte en 3D (nouveau).
  - [Face Swap Any Video](https://youtu.be/XwaLYUjWaEg?si=JtLFSmNSP4jJ4kTy)
  - √âdition et g√©n√©ration vid√©o simplifi√©es et am√©lior√©es :[Motion Follower, Easy Animate et Uni Animate](https://chatgpt.com/share/3230e7bc-3f2c-46e1-8d76-20e9468eb2f0)

      **MotionFollower : √âdition de mouvement vid√©o par IA**
      [En savoir plus](https://francis-rings.github.io/Motio...)
      Avez-vous d√©j√† souhait√© pouvoir transf√©rer des mouvements d'une vid√©o √† une autre de mani√®re transparente ? MotionFollower est l√† pour r√©aliser vos r√™ves ! Ce mod√®le d'IA de pointe vous permet de transf√©rer les mouvements d'une vid√©o source √† votre protagoniste, tout en pr√©servant l'arri√®re-plan et l'apparence originaux. Avec son pose controller, reference controller, score-guided diffusion et son architecture efficace, MotionFollower surpasse les autres mod√®les en termes d'utilisation de la m√©moire GPU et de gestion des arri√®re-plans complexes et des mouvements de cam√©ra.
      
      **EasyAnimate : G√©n√©ration avanc√©e de vid√©os par IA**
      [En savoir plus](https://github.com/aigc-apps/EasyAnimate)
      Pr√©parez-vous √† √™tre √©poustoufl√© par EasyAnimate, un mod√®le d'IA avanc√© d√©velopp√© par les brillants esprits du groupe Alibaba. En utilisant l'architecture Transformer et le cadre Diffusion Transformer, EasyAnimate peut g√©n√©rer des vid√©os de haute performance et de longue dur√©e avec des transitions de mouvement fluides. Sa polyvalence, son approche Slice VAE et son √©cosyst√®me holistique le rendent adapt√© √† diverses applications, des images aux vid√©os. Ce mod√®le d'IA √©tablit une nouvelle norme en mati√®re de g√©n√©ration de vid√©os par IA et ouvre des possibilit√©s infinies pour les cr√©ateurs.
      
      **UniAnimate : Animation d'images humaines r√©alistes**
      [En savoir plus](https://unianimate.github.io/)
      [GitHub](https://github.com/ali-vilab/UniAnimate)
      UniAnimate est le r√©volutionnaire de l'animation d'images humaines que vous attendiez ! Avec son mod√®le de diffusion vid√©o unifi√©, UniAnimate peut animer une seule image de r√©f√©rence bas√©e sur une s√©quence de poses cibles, produisant des vid√©os √©tonnamment r√©alistes. L'entr√©e de bruit unifi√©e du mod√®le et la mod√©lisation temporelle efficace assurent des transitions fluides et une gestion efficace des s√©quences plus longues. Les r√©sultats de synth√®se sup√©rieurs d'UniAnimate et sa capacit√© de transfert inter-domaines en font le mod√®le d'IA incontournable po

32. **Automatisation de t√¢ches**
- [Fabric](https://github.com/danielmiessler/fabric)
  - Voici une liste des principales fonctionnalit√©s de Fabric, le framework open-source d'IA :
    ‚Ä¢ üìù Extraire les points cl√©s et r√©sumer des vid√©os YouTube, podcasts et longs contenus
    ‚Ä¢ ‚úçÔ∏è R√©diger des essais ou textes dans votre propre style √† partir d'une simple id√©e
    ‚Ä¢ üìö Synth√©tiser des articles acad√©miques complexes
    ‚Ä¢ üé® G√©n√©rer des prompts d'IA pour cr√©er des images correspondant √† un texte
    ‚Ä¢ ‚≠ê √âvaluer la qualit√© d'un contenu avant de le lire/regarder en entier
    ‚Ä¢ üíª Expliquer du code et am√©liorer de la documentation technique
    ‚Ä¢ üìä Analyser des donn√©es et extraire des insights
    ‚Ä¢ üó£Ô∏è Cr√©er des posts pour les r√©seaux sociaux √† partir de n'importe quel contenu
    ‚Ä¢ üîç Extraire des citations et passages pertinents de diff√©rentes sources
    ‚Ä¢ üìÖ Analyser vos habitudes et routines √† partir de vos notes et donn√©es personnelles
    ‚Ä¢ üß† R√©soudre des probl√®mes en les d√©composant en sous-t√¢ches
    ‚Ä¢ üîÑ Combiner plusieurs prompts (appel√©s "Patterns") pour des t√¢ches plus complexes
    ‚Ä¢ üõ†Ô∏è Cr√©er vos propres prompts personnalis√©s pour des besoins sp√©cifiques
    ‚Ä¢ üó£Ô∏è Interagir par commandes vocales ou interface en ligne de commande
    ‚Ä¢ üîó S'int√©grer avec d'autres outils comme Obsidian pour la prise de notes
- [vectorShift](https://vectorshift.ai/) : Un √©cosyst√®me pour concevoir, d√©ployer et g√©rer des applications d'IA en no code. [D√©couvrez](https://www.youtube.com/watch?v=Pjdnq-WcDHQ) comment cr√©er un chatbot d'IA en utilisant VectorShift, capable de rechercher et de r√©pondre aux requ√™tes bas√©es sur le contenu de votre site web !

33. **Comment am√©liorer les performances d'un LLM**:
    
![image](https://github.com/user-attachments/assets/0e698b2d-b1cc-4e3b-bbc3-769a45dbec41)


