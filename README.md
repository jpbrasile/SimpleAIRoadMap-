

__***Avec le monde d'avant l'IA on apprenait √† faire puis on faisait. Avec le monde d'aujourd'hui on fait faire √† l'IA, puis on apprend √† partir de ce que l'IA a fait, on finit donc par savoir faire aussi. Du coup plus besoin de prof, il ne suffit que de vouloir faire pour parvenir √† nos fins !! The sky is the limit !!!***__


# [SimpleAIRoadMap](https://chatgpt.com/share/d2c569e9-b863-4143-959c-583dc10b6257) *:
*: Nous avons mis un hyperlien sur le chat d'aide √† la production de ce markdown 

# **How to master AI step by step from beginner to expert level**

_**Savoir Lire, Ecrire , Compter : A l'heure de l'IA, ses savoir-faire de base se sont √©tendus:**_

- **Lire** c'est aussi naviguer sur le web (_web scraping_) et apprendre en regardant des vid√©os sur YouTube (_video reading_) 
- **Ecrire** c'est aussi chatter (_prompting_) avec l'IA (_LLM_) , envoyer des mails(_automation_), cr√©er des vid√©os (_video_making_) ou utiliser un espace de stockage qui nous est propre et facilement accessible (_RAG_). C'est aussi savoir communiquer avec notre environnement (_multimodal_)
- **Compter** c'est aussi lancer des programmes (_coding_), acc√©der √† des ressources externes(_API)_

L'IA doit aussi savoir s'adapter au niveau de l'utilisateur :

- Pour le __d√©butant__, elle fournira des "bo√Ætes noires" qui r√©aliseront certaines des t√¢ches d√©crites plus haut sans que l'utilisateur ait √† faire d'effort. C'est le _"What does it do ?"_.
- Pour l'__apprenti__, elle permettra de r√©aliser des t√¢ches plus complexes en associant les outils disponibles. C'est le _"How to use it ?"_.
- Pour celui qui __ma√Ætrise__, il adaptera les bo√Ætes existantes √† ses besoins. C'est le _"How to improve what it does ?"_.
- Pour l'__expert__ enfin, il cr√©era ses propres outils. C'est le _"How to create new tools ?"_.

Si toutes ces capacit√©s sont int√©gr√©s dans notre IA, elle devient un parfait assistant 
D'autres contraintes doivent √™tre int√©grer durant notre parcours: Souhaite-t-on des solutions locales ou d√©port√©es,payantes ou gratuites ?

- [**Classification des possibilit√©es de l'IA**](https://chatgpt.com/share/c7d34658-7c48-4e70-ad22-01c3d5071553) 

  - **Data Acquisition** :Text/Voice/Image/Video/Vision/Web Scraping/External API 
  - **Data Processing** : Prompting/RAG/LLM/multimodal LLM/Reasoning/Automation/Tools creation/Object manufacturing 
  - **Creation** :Text/Voice/Image/Animation/Video/Web App/3D modeling/API Endpoint Development/Virtual Operator/Fabrication
  - **Tools** : Multimedi treatment/Function calling/Agents/Avatar/ChatBot/Voice assistant/Coding assistant/Data scraping assistant/Teaching Assistant/Project Assistant/Remote phone assistant/Website Assistant Chatbot/Expert assistant


<br><br>
![image](https://github.com/user-attachments/assets/7d25f8b8-77d5-4b30-b940-4adfbe331e31)
<br><br>
- Le coeur du syst√®me, les **LLM** (Large language model)  recoivent du texte (ou de plus en plus des sources d'origines vari√©es), le traite et fournissent du texte en retour. Le texte d'entr√©e doit √™tre tel qu'il exprime clairement et concr√®tement nos attentes (c'est le __prompting__)
- Le texte en retour peut √™tre format√© pour correspondre √† une r√©ponse de type texte brut, JSON,  markdown, HTML , code , API ... suivant le post processing envisag√©.
<br><br>


| Data Acquisition            | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Text                        |üõ†Ô∏è [NotebookLM](https://notebooklm.google/) -<br>[ses caract√©ristiques](https://chatgpt.com/share/2a77dd27-ca9c-41ef-a85e-f401362111cf),<br>üõ†Ô∏è [Perplexity](https://www.perplexity.ai)         |               |  [DumpDir](https://www.perplexity.ai/page/how-to-use-dumpdir-iZmgCyuZTb6EsC0rxQt_LQ)        |        |
| Voice                       |          |               |          |        |
| Image                       |    [free database](https://www.pexels.com/)      |               |          | [Full control with CGDream](https://www.perplexity.ai/page/cgdream-synthese-en-francais-RYRbYJrySjKNyKf6mrvOEQ)       |
| Video /Vision               | [Object detection](https://claude.site/artifacts/58f9617b-29ff-4d72-94c6-fef471fe4550)         |               | [openedai-vision](https://github.com/matatonic/openedai-vision)         |        |
| Web Scraping                |üõ†Ô∏è [Perplexity](https://www.perplexity.ai) <br>üõ†Ô∏è [Harpa](Harpa.ai)          | [serper](https://serper.dev/)              |  [gpt researcher](https://www.perplexity.ai/search/remet-en-forme-en-faisant-si-n-sATjIynuR9i6G8ikas0w2w)   <br> [scrape anything](https://www.perplexity.ai/page/web-scraping-OW_sI2YiTj.O.6yk4kQuJQ)     |        |
| External API     |          |               |          |        |

<br><br>


| Data Processing             | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| [**Prompting**](https://chatgpt.com/share/1bc7eb5b-5afd-4d57-a963-5fc3b73ae1fa)                   |- [**mise √† jour des LLM****](https://www.perplexity.ai/search/vous-etes-un-assistant-ia-spec-yFTongczR5q288uYziNP_A) <br> [**Chain of thought](https://chat.mistral.ai/chat/f035abe1-8249-402e-90a8-031ebc7c28d1) <br> [Metaprompt](https://chatgpt.com/share/9bf5e118-0943-4d83-98fa-40b4ade83247)          |               |          |        |
| **sauvegarde ChatBot**                   |   [en local (markdown)](https://chromewebstore.google.com/detail/save-my-chatbot-ai-conver/agklnagmfeooogcppjccdnoallkhgkod       |               |          |        |
| RAG (Retrieval-Augmented Generation) |          | [Colpali: avec vision](https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/)              |  [Tutorial 1](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Advanced%20RAG%20A-Z%20Course) <br> [Graph RAG tutorial](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb)        |        |
| LLM (Large Language Models) |[Sonnet Chat](https://claude.ai/new)<br>[ChatGPT](https://chatgpt.com/)          | [OpenAI playground & API](https://platform.openai.com)  <br> [Deepseek api](https://www.deepseek.com/) <br> [Groq](https://groq.com/)  <br> [Mistral](https://mistral.ai/) <br> [Sonnet API](https://www.anthropic.com/api-bk)          |  [mathstral](https://mistral.ai/news/mathstral/)        |        |
| **LLM Multimodal** | sonnet 3.5, GPT4o,GPT-4o mini, [Groq1.5](https://www.perplexity.ai/page/elon-musk-s-xai-and-grok-0c66ZeWOT6SS4tCeiy3H6Q)         |   [GPT-4o mini API](https://mail.google.com/mail/u/0/#inbox/WhctKLbFVzmwlxRsPCWTsjMLZZvQVkVGzGLLKXFHWsWvVshssjsmzDQqSZKTqKzXxhWSctL) <br> [-HuggingChat](https://www.perplexity.ai/page/capacites-de-huggingchat-mt909eRSSt2sBHKBWqO71Q)           |          |        |
| **Reasoning**                   |          |               |          |        |
| **Automation**                  |          |               |          |[Fabric](https://github.com/danielmiessler/fabric) <br> - [ses fonctionnalit√©s](https://chatgpt.com/share/1f411711-2457-459f-94d8-2d3d67805592)       |
| **Tools creation**              |          |               |          |        |
| **Objects manufacturing**            |          |               |    [text to real 3D part](https://chatgpt.com/c/847ba680-fd83-4ed7-bd0b-84a2d75f0f67) 
   

<br><br>


|  Creation   | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| **Text**                        |          |               |          |        |
| - Transcription audio mulitlangues, multi partcipants                        |          |[aTrain](https://github.com/JuergenFleiss/aTrain)               |          |        |
| **Voice**                       |  [full control,free](https://ttsopenai.com/]      |    [alltalk_tts](https://github.com/erew123/alltalk_tts) <br> [~ elevenlabs mais gratuit](https://ttsopenai.com/) <br> [Chansons avec suno](suno.ai) <br> [CosyVoice en local, facile √† implanter](https://chatgpt.com/share/881feabd-9e38-4eea-8776-6e3bcafe7359)          |   [en local gratuit](https://www.perplexity.ai/page/summarize-in-french-dNV1x459RlCo3CJ.pJvuFA)       |        |
| **Image**                       | [leonardo](https://www.perplexity.ai/page/amelioration-de-leonardo-ai-Du0LlRT3T6OVeM0k_hwM_g) <br>[Fooocus on Colab](https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb)  <br>  [fooocus api](https://replicate.com/konieshadow/fooocus-api/api)  <br> <img src="https://github.com/user-attachments/assets/ae951b46-0b30-46da-b940-44b34f5af0fe" width="60%"> <br> [tout en un pour 9‚Ç¨/mois](https://www.perplexity.ai/search/donne-moi-les-fonctionnalite-d-zy4EKgaqQEi2VjSn.fAMvQ) <br>  [AuraFlow](https://www.perplexity.ai/search/resume-sous-forme-de-liste-a-p-95pSiRbEQ3e9vCU9KCVe9w) <br> - en playground pour 0.02 ‚Ç¨ <br> <img src="https://github.com/user-attachments/assets/1cd6927e-2ad4-49d3-9c52-99aeff69700b" width="25%">  | [CompfyUI sur HF](https://huggingface.co/spaces/Deadmon/union-sdxl): <br> Prompt: En mode manga : Night_sky_1 with a full moon. Piper_McNimbus (12 years old, long auburn hair, freckles, bright green eyes, wearing pajamas,) soars through the air, arms outstretched, with a joyful expression. Clouds_1 (wispy, moonlit) surround her. Stars_1 (twinkling, numerous) fill the background. The atmosphere is magical and liberating. <br> Image: <img src="https://github.com/user-attachments/assets/12e8187c-c43e-402e-9182-a4f2a3511136" alt="Generated Image" width="50%"> <br> [- RunCompfy)(https://www.perplexity.ai/page/resume-en-francais-des-capacit-R6iz0DSBSI6GcB2cz.oRNQ) |
| -avec choix du style | [tengr](https://tengr.ai/) <br> <img src="https://github.com/user-attachments/assets/16a40593-5360-473f-8d50-c131e6b35312" width="50%"> |
| - upscaling                       |  [Upscaling](https://flowt.ai/community/supir-v2-plugandplay-edition-n5acf-v)       |               |          |        |
| - avec Caract√®res consistants                       |          |   [FaceChain](https://facechain-fact.github.io/)  <br>   [portrait/v√™tements/prompt pour la pose](https://arxiv.org/html/2406.09162v1)         |          |        |
| **Animation**                   | [hedra](https://www.hedra.com/)         |               |          |        |
| - avec images humaines r√©alistes    |        |               |  [unianimate](https://unianimate.github.io/)         |        |
| - animation entre 2 images| | |[avec Luma Dream Machine ](https://www.perplexity.ai/page/animate-any-characters-lpRGS7C9StuHQK8stTS6uQ) |
| - √† partir d'une image   |        |               |   [EasyAnimate](https://github.com/aigc-apps/EasyAnimate)        |        |
| - √† partir de 2 vid√©os    |        |               |  [MotionFollower]([https://unianimate.github.io/](https://francis-rings.github.io/MotionFollower/))         |        |
| **Video**                       |          | [conseils pratiques](https://www.perplexity.ai/page/creer-une-video-d-image-iY8wTR3dRMWBNygLeSFjNw)             |          |        |
| - avec caract√®res persistants                       |          |  [ArtFlow](https://app.artflow.ai/character-builder?feature=actors)             |          |        |
| - avec √©motions                       |          |  [https://liveportrait.github.io/](https://www.perplexity.ai/page/resume-en-francais-sc4ilfT4TSOUHajqxjh4Rg)            |          |        |
| - Face swap                       |          | [Face Swap Any Video](https://www.perplexity.ai/search/resume-sous-l-la-forme-d-une-l-u3JLXzacT9m06yn8xR3c2w) <br> [et aussi](https://www.perplexity.ai/page/resume-emoji-youtube-video-DIRYmQpaSay7rPGEhxsvEg)           |          |        |
| - de longue dur√©e   |          | [open source](https://video-infinity.tanzhenxiong.com/)           |          |        |
| - bruitage automatique   |          |           |          |  [open source](https://github.com/open-mmlab/FoleyCrafter)      |
| **Web app**                     |          |  [Websim: Text 2 Web](https://www.perplexity.ai/page/websim-ai-que-fait-il-RUuskkooQkKt84xNKISK8A)             |          | [Scientific: client side](https://www.lgstoolkit.com/apps/trusses/) <br> üí°: digital twin client side         |
| **3D modeling**                 | [ERA3D sur colab & Kaggle](https://github.com/wandaweb/Era3D-Colab-Kaggle/)         |     [wonder3D](https://github.com/xxlong0/Wonder3D) <br> <img src="https://github.com/user-attachments/assets/86f13894-488c-4d69-b86d-97e49b3b7b1f" width= "70%">|          |  [CadQuery](https://github.com/CadQuery/cadquery) <br> -[ses caract√©ristiques](https://chatgpt.com/share/c6743a29-56b2-4536-84fa-209041d3ff02)  <br> [meshi: impressionnant (free API)](https://www.meshy.ai/) <br> - [onshape :free API, scaling](https://www.onshape.com/en/pricing)    |
| **API Endpoint Development**    |          |  [Gorilla](https://gorilla.cs.berkeley.edu/)              |          | [vectorShift](https://vectorshift.ai/)    |
| **Virtual Operator**            |          |               |          |        |
| Local Development Environment      |[Github](https://github.com/) <br> [- Tuto web site deploy & cheat sheet](https://www.perplexity.ai/search/creer-son-site-en-ligne-avec-g-vCw21czUT6.9nE7dSYjeHA) <br>[LMStudio](https://lmstudio.ai/)         |  [Lightning AI](https://chatgpt.com/share/05b67f08-a0da-495c-a295-198e63e6c9bd) <br> [HuggingFace candle](https://www.perplexity.ai/search/faire-un-resume-de-https-www-y-Nd7V1Ag9TjGjS9gY0AJCgQ)  <br> [Docker](https://www.docker.com/)            |          |        |
| **Cloud based Development Environment**            | [colab.google](https://www.perplexity.ai/search/quelles-sont-les-principales-c-NcNfvE5bRASAqlxa6IoQQA) <br> [kaggle.com](https://www.perplexity.ai/search/quelles-sont-les-principales-c-eZGY4KOjRyOxAXcPjfMKVA) <br> [huggingface.co](https://www.perplexity.ai/search/https-huggingface-co-quelles-s-45BlLQtASYGCMo4QEsGixA) <br>         |               |          |        |
| **Fabrication**             |          |               |          | - [Robot <1000 ‚Ç¨](https://github.com/Red-Rabbit-Robotics) <br>[- Imprimante SLS DIY](https://sls4all.com/store/)       |
| **Text 2 Graph**             |          |               |          | [https://www.youtube.com/watch?v=4v42JHuI30Y](https://www.perplexity.ai/page/resume-en-francais-P2yjJ105TTGAXbe8cw5gVQ)     |

<br><br>

| Tools                  | D√©butant | Interm√©diaire | Ma√Ætrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| multimedia Treatment(texte,...3D)                  |  [Viva]( https://vivago.ai/home)        |               |          |        |
| Function calling                |  | [Gorilla](https://gorilla.cs.berkeley.edu/) |[best LLM](https://www.perplexity.ai/page/function-calling-resume-en-fra-liyyJb_cSwmUGkmHxo9Gsg)  |  |
| **Agents**                      |          |               |  [Build agent with Docker](https://www.perplexity.ai/search/resume-sous-la-forme-d-une-lis-4bHWdgdFRo6exzkPiyX0ag) <br> [agents on internet](https://www.perplexity.ai/search/explique-sous-forme-de-liste-a-h0KnUnaSSFyQucTGJuh2Ag)  <br> [agency swarm](https://www.perplexity.ai/page/synthese-en-francais-youtube-BqhFkU6iRe6xw3_n3.xOrQ) <br> [colab](https://colab.research.google.com/drive/1EpskNFHPvWQV09MBOO8uwJcLWoJZSa1X?usp=sharing)     |        |
| Avatar                      |          |  [via Kaggle](https://www.youtube.com/watch?v=pvGhSfsi1Ys)             |          |        |
|  - parlant ou chantant                      |          |  [Gratuit et bluffant](https://github.com/fudan-generative-vision/hallo?tab=readme-ov-file)  !            |          |        |
| Chatbot          | [pmfm](pmfm.ai) <br> - [ses caract√©ristiques](https://chatgpt.com/share/3599eac7-d41b-484a-bb9b-1929482a3f56)         |  [Poe.com](https://www.perplexity.ai/search/chatbot-https-poe-com-principa-xeo_mfVZRZSmg.MJYxnibA)        |          | [Creation](https://www.perplexity.ai/search/resume-sosu-forme-de-liste-a-p-8yTUCkJeSpyyfWlCSPDOEA)       |
| - Voice assistant                       |          |               |[en local](https://www.perplexity.ai/search/resume-dans-unne-liste-a-puce-94GkUCHiTFOwvm9_Mzsk2A) <br> [Multimodal](https://www.perplexity.ai/search/resume-dans-une-liste-a-puces-wT0ANkSQRxmb3d5SCrF_vA)) <br> [Allice](github.com/myshell-ai/AIlice) <br> - [sescaract√©ristiques](https://chatgpt.com/share/dd1851ef-4020-4ffe-842b-249d630c8ac9)
  |        |
| **Coding assistant**            |          |[claude engineer](https://github.com/Doriandarko/claude-engineer) <br> [aider.chat](https://www.perplexity.ai/search/aider-code-assistant-cheat-she-REvsq3zxSwS75PPD_HcVeQ)              |          |        |
| Data scraping assistant     |          |  [crawl4ai](https://github.com/unclecode/crawl4ai) <br> - [ses caract√©ristiques](https://chatgpt.com/share/13f51641-c3d2-4b57-af30-ffb3b31e068f)             |          |        |
| Teaching Assistant          |          |               |          |        |
| Project assistant           |          |               |          |        |
| Remote phone assistant      |          |               | [VAPI](https://docs.vapi.ai/introduction)         |        |
| Website assistant Chatbot      |          |               |         |  ‚¨ú: [vectorShift](https://www.youtube.com/watch?v=Pjdnq-WcDHQ)      |
| Expert assistant            |          |               |          |        |
<br><br>
  



<br>

<br>


## AI-automation
Tout faire avec l'IA. Elle fait le boulot sous votre contr√¥le et vous forme √† comprendre comment tout √ßa fonctionne.

L'id√©e est de construire pas √† pas une "baquette magique" apte √† tout faire. Nous nous bornerons √† vous indiquer:
- Les bons outils √† utiliser,
- Comment bien poser votre probl√®me
- Comment apprendre cette nouvelle fa√ßon de travailler , en comprenant comment la magie op√©re.
- Cela passe par l'emploi d'outils pour :
  - Ne pas r√©inventer la roue, donc voir si notre probl√®me n'est pas d√©j√† r√©solu. c'est le "**web scraping**" en particulier sur Github, Hugging Face ou Papers with Code
  - R√©cup√©rer l'√©tat de l'art c'est la combinaison de "web scraping" de site comme Arxiv ou Github en particulier et le **RAG** pour stocker le savoir
  - Le programmer en "no code" avec les **coding assistants**
  - Utiliser des **outils** locaux ou distants (via des**API**), des **agents**, dot√© d'intelligence, d'une mission , de m√©moire et d'outils d√©di√©s)  collaborant entre eux pour r√©soudre les probl√®mes les plus complexes, chaque fois qu'une programmation directe s'av√®re difficile 


## Les √©tapes √† franchir pour automatiser
- Mettre en oeuvre un chat √† base de LLM (_d√©butant_), c'est le coeur du syst√®me : Sonnet 3.5 est le meilleur du moment, en particulier dans sa version 20 ‚Ç¨ par mois qui offre la fonction Artefacts
- Acc√©der au LLM via des API tierces (_d√©butant_) ou cr√©√©e par nous m√™me (_apprenti_) : c'est le d√©but de la programmation par "no code"
- Exemple d'outils pour [appeler des fonctions externes](https://gorilla.cs.berkeley.edu/leaderboard.html)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/084708bc-b8f1-469e-94bd-32d48cc6cf50" width="600" />
<br>
- Le chainage d'API, associ√© au no code, permettra √† l'_apprenti_ de cr√©er des fonctions complexes (comme la cr√©ation de vid√©o √† partir de texte brut)

- Dans ce contexte acc√©der (_apprenti_) - ou d√©velopper (_intermediaire_) - des API pour le web scrapping, la m√©morisation d'un large corpus de donn√©es, et pour disposer d'un coding assistant est une priorit√© <br>

- On passe alors √† la cr√©ation de fonctions plus ambitieuses (_master_):
  - La cr√©ation d'objet 3D param√©trique √† partir d'un chat avec un LLM
  - La cr√©ation de graphes √† partir d'un chat avec un LLM (story board, graphes de connaissances)
  - le portage en local, open source de toutes les briques utiles (avec un acc√®s externes via des API
  - La cr√©ation d'agents d√©di√©s √† la r√©alisation de t√¢ches complexes (_expert_)
    - Teaching Assistant
    - Coding assistant
    - CAD assistant       



## Cr√©ation automatique d'une API web qui peut effectuer deux op√©rations math√©matiques : (_apprenti_)

- Additionner deux nombres
- Multiplier deux nombres

- L'application doit √™tre conteneuris√©e avec Docker pour faciliter son d√©ploiement et son ex√©cution.
- Vous voulez un guide √©tape par √©tape pour cr√©er cette application, en partant de z√©ro, sans aucun outil pr√©install√© sur votre ordinateur.
- L'objectif final est de pouvoir non seulement cr√©er cette application, mais aussi de pouvoir la partager facilement. 
- Vous voulez que n'importe qui puisse la t√©l√©charger et la lancer sur son propre ordinateur, quelle que soit sa configuration.
- Vous avez besoin d'instructions claires sur comment lancer l'application et comment la reproduire sur un autre PC.
- [**Dialogue avec sonnet 3.5** pour mettre en oeuvre la solution en "manuel"](https://claude.ai/chat/a71daeb6-5875-4ecb-9dc6-7dce126afde0) 
- Nous verrons plus tard comment automatiser la mise en place de ce type d'application en automatique avec AIDER
  
### Faisons le tutoriel correspondant sous forme de vid√©o (_interm√©diaire_)
- L'id√©e est de partir de la synth√®se r√©capitul√©e par sonnet 3.5 de notre programme pr√©c√©dent pour en faire un tuto.
- Pour cela on √©tablit un [dialogue avec sonnet 3.5](https://claude.ai/chat/08fb3cc8-5cb4-45ed-9132-953e30ecf792) pour d√©grossir le probl√®me:
    - Pour cr√©er les planches HTML support,
    - Le texte des voice over,
    - Le prompt pour produire les images
    - Le code python pour stocker ces donn√©es dans des r√©pertoires
    - Le code python cr√©ant les MP3 et les images et qui les stockent
    - Le code python qui fait l'assemblage
- Ce d√©grossissage montre qu'il est pr√©f√©rable d'avancer pas √† pas en construisant et validant pas √† pas le code python correspondant, ce que nous allons faire maintenant avec un nouveau thread sonnet 3.5.
  
## Cr√©er un code python permettant de dialoguer avec sonnet 3.5
  - On utilisera Visual Studio Code pour la mise en oeuvre et pour tester les codes
  - On utilisera anaconda pour cr√©er un environnement logiciel sp√©cifique. Nous utiliserons l'environnement teambot d√©j√† cr√©er avec `conda activate teambot` dans un terminal 
  - On cr√©e un r√©pertoire de travail video-maker dans lequel on met le fichier .env avec nos clefs API, ainsi que les fichiers requirements.txt et anthropic-api-hello-world.py cr√©er par sonnet 3.5
  - Le dialogue fonctionne :
    
```bash
(base) PS C:\Users\test\Documents\AI_Automation\video_maker> conda activate teambot

(teambot) PS C:\Users\test\Documents\AI_Automation\video_maker> python anthropic-api-hello-world.py
Claude dit: [TextBlock(text='Bonjour !', type='text')]
```

## Cr√©ation d'une vid√©o √† partir d'un texte](https://claude.ai/chat/c33dece9-e5ab-4206-98c6-de644cb1d731)  
- Ce projet automatise la cr√©ation de vid√©os √©ducatives √† partir de contenu textuel, utilisant diverses technologies et APIs. Le processus se d√©roule en plusieurs √©tapes int√©gr√©es dans un script Python unique :
  - Conversion du texte :
    - Lit le contenu du fichier PLACE_HOLDER_TEXTE_VIDEO.txt.
    - Utilise l'API Claude d'Anthropic pour convertir le texte en structure JSON de diapositives.
  - Traitement des diapositives :
    - G√©n√®re un fichier HTML structur√© avec CSS int√©gr√© pour chaque diapositive.
    - Cr√©e un texte de voix off avec Claude.
    - Produit une image illustrative via l'API DALL-E d'OpenAI.
    - G√©n√®re un fichier audio de la voix off avec l'API Text-to-Speech d'OpenAI.
  - Cr√©ation des vid√©os :
    - Capture une image du HTML rendu avec Selenium.
    - Combine l'image et l'audio en utilisant MoviePy pour chaque diapositive.
  - Agr√©gation finale :
    - Assemble toutes les vid√©os individuelles en une seule vid√©o.
    - Ajoute des transitions entre les diapositives.
  - Le projet utilise Python avec diverses biblioth√®ques (BeautifulSoup, Requests, Pillow, MoviePy) et APIs (Anthropic, OpenAI). Cette approche int√©gr√©e offre une solution compl√®te et efficace pour la production automatis√©e de contenu vid√©o √©ducatif, de la conversion du texte √† la cr√©ation de la vid√©o finale.


4. **Point d'√©tape:**
- Nous avons r√©ussi √† mettre en oeuvre une applicatoin complexe sans coder une seule ligne. Cependant ce faisant nous avons d√©tect√© des pistes pour augmenter encore notre productivit√©
  - **Automatiser les it√©rations de d√©buggage** , ce qui nous a fait perdre le plus de temps dans la mise au point du code
  - Passer √† l'open source, en particulier pour la cr√©ation d'image qui constitue le poste de d√©pense le plus √©lev√© pour la cr√©ation d'une vid√©o
  - Faire du web scraping pour voir si notre probl√®me n'est pas d√©j√† r√©solu par ailleurs
 
5. **Coding assistant**:
- ‚úÖ: AIDER+Sonnet**
  - On cr√©e le r√©pertoire ```coding_assistant``` et on lance ```conda activate teambot```
  - On suit les [instructions d'installation](https://github.com/paul-gauthier/aider) 
  - Mais il faut l'adapter au terminal powershell :```$env:ANTHROPIC_API_KEY="sk... "```
  - AIDER r√©pond √† nos directives et adapte en cons√©quence un repository qui a √©t√© clon√© localement.
  - Il conserve un logbook des actions entreprises (```.aider.chat.history.md```) et un LLM comme Sonnet 3.5 ou GPT-4o peut alors en faire la synth√®se:
    ```J'ai eu des probl√®mes qui ont √©t√© r√©solu dans le document joint, fais en la synth√®se```
  - AIDER est [SOTA](https://aider.chat/docs/leaderboards/) avec Sonnet 3.5 mais il est aussi tr√®s performant avec DeepSeekCoder
  - ‚¨ú :[AIDER avec deepSeek](https://youtu.be/Y-_0VkMUiPc?si=zAPZQrYj6yrMHhne)
-  ‚¨ú [**Micro-agent**](https://github.com/BuilderIO/micro-agent) : it√©ration automatique sur des cas tests
-  ‚¨ú [**CodeGeeX4**](https://github.com/THUDM/CodeGeeX4) : en plugin de VS code

- 
6. [**Text to image local dans docker**](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
- Il faut "alimenter" comfyui en y rajoutant les chkpoints requis √† placer dans le r√©pertoire : ```C:\Users\test\Documents\AI_Automation\coding_assistant\comfyui\storage\ComfyUI\models\checkpoints```
- Il n'y a pas de consensus clair sur un seul ¬´ meilleur ¬ª point de contr√¥le pour ComfyUI, car cela d√©pend beaucoup de vos pr√©f√©rences personnelles et du type d'image que l'on souhaite g√©n√©rer. Cependant, plusieurs points de contr√¥le sont fr√©quemment recommand√©s pour leur qualit√© :
  - SDXL (Stable Diffusion XL) : C'est un mod√®le de base tr√®s performant, particuli√®rement bon pour le r√©alisme et la qualit√© g√©n√©rale des images.
  - Juggernaut XL : Souvent cit√© comme l'un des meilleurs pour le photor√©alisme
  - Dreamshaper : Appr√©ci√© pour sa polyvalence et sa qualit√©, particuli√®rement dans sa version Turbo
  - Vision r√©aliste : Excellent pour g√©n√©rer des humains r√©alistes.
  - RealVis XL : √âgalement recommand√© pour le photor√©alisme
- La g√©n√©ration d'images peut se faire via une requ√™te API comme le montre  [basic_api_exemple.py](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)


  8- **Cr√©er un story teller automatique**
- [SEED](https://github.com/TencentARC/SEED-Story) : SEED-Story, un mod√®le de langage multimodal (MLLM) capable de g√©n√©rer de longues histoires multimodales compos√©es de textes narratifs riches et coh√©rents, accompagn√©s d'images coh√©rentes en termes de personnages et de style, bas√© sur SEED-X. Avec StoryStream, un vaste ensemble de donn√©es sp√©cialement con√ßu pour l'entra√Ænement et l'√©valuation comparative de la g√©n√©ration d'histoires multimodales.
<img src="https://github.com/user-attachments/assets/8b2bb448-b9ff-452d-b316-885dda145ae9" width=350%">

- La premi√®re √©tape consiste √† produire des images consistantes afin que les personnages ou objets reproduits sur plusieurs diapositives soient quasi-identiques:
  - Nous avons cr√©√© un "m√©ta-prompt" qui √† partir d'un descriptif simple <situation> fournit des images consistantes:
  - M√©ta-prompt:

```
Prompt_Structure:
{scene_setting}. {entity1_name} ({entity1_traits}) {action1} {object1_name} ({object1_traits}). {entity2_name} ({entity2_traits}) {action2}. {atmosphere}

Definitions:
- scene_setting: Brief description of location and context
- entity1_name: Identifier for the first entity (e.g., man_1, woman_1, dog_1)
- entity1_traits: age, physical_appearance, clothing
- action1: Main action of entity1
- object1_name: Identifier for the main object (e.g., watch_1, map_1)
- object1_traits: material, distinctive_features, size
- entity2_name: Identifier for the second entity
- entity2_traits: age, physical_appearance, clothing/characteristics
- action2: Main action of entity2
- atmosphere: General ambiance and shared activity

Instructions:
1. Replace each element with specific details consistent with the provided <situation>.
2. Maintain coherence between entities, their actions, and the context.
3. Use unique identifiers for entities and objects to facilitate reference and reuse.

Example:
Cafe_table_1 in a bustling coffee shop. Man_1 (25 years old, short brown hair, white shirt and blue jeans) carefully examines Watch_1 (antique gold, floral engravings, normal size). Woman_1 (70 years old, gray hair in a bun, red floral dress and round glasses) leans forward with interest. They are absorbed in their animated conversation.
```
- Sonnet a r√©alis√© le prompt sp√©cifique √† partir de ```<situation> A man walking his dog in the forest```
```
Forest_trail_1 in a lush, shadowy woodland with dappled sunlight. Man_1 (45 years old, salt-and-pepper hair, rugged stubble, green waterproof jacket and khaki hiking pants) studies Map_1 (weatherproof paper, colorful trail markings, slightly creased) while striding purposefully. Dog_1 (6 years old, German Shepherd, glossy black and tan coat, red nylon collar with tags) trots eagerly ahead, nose twitching at the scent of pine and earth. They are enveloped in a serene atmosphere of natural exploration, with a gentle breeze rustling through the canopy above.
``` 
  - Voil√† le r√©sultat fournit par copilot pour la cr√©ation d'image avec copilot:
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/497bf3b4-b95d-451a-8775-1c99a2f5ac5d)
  - et avec leonardo.ai (qui oublie la carte et met le "collier rouge" sur le vieil homme) :
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/ed959f12-0774-472a-950f-e810baa0c861)

  - La deuxi√®me √©tape consiste √† avoir le script de la vid√©o, c'est √† dire tous les √©l√©ments textuels qui permettront la cr√©ation automatique de la vid√©o.  
    - Trouver des id√©es d'un livre : le prompting est essentiel pour y parvenir: [`trouve dix livres int√©ressants pour un enfant de 12 ans sur les super h√©ros`](https://www.perplexity.ai/search/trouve-dix-livres-interessants-krUHHsFMRMGL1qYUI5.1yQ)
    - Le livre √©tant choisi on r√©cup√®re la trame en alimentant sonnet 3.5 de toutes les informations r√©cup√©r√©es sur  le net et en lui demandant [`imagine le script de ce livre, en le structurant sous une forme compacte et structur√©e qui sera lu uniquement par un llm pas par un humain. utilise les ressorts classiques d'√©criture pour ce style de livre`](https://claude.ai/chat/72358672-7eb9-416a-a16a-dcad3a129953) puis la suite de la discussion avec sonnet 3.5 nous permet de g√©n√©rer un fichier story.JSON qui contient √† la fois le voice over et le descriptif des images en conservant la consistance des personnages.
    - Nous allons maintenant utiliser AIDER pour transformer le programme apte √† g√©n√©r√©er des tutoriels vid√©o √† ce nouveau besoin.
      - Nous avons synchronis√© notre d√©pot video_maker √† github afin que AIDER puisse le prendre en compte
      - Nous avons cr√©er une appli qui lance AIDER dans l'environnement conda teambot avec `python launch_aider.py` . Cela permet de r√©cup√©rer la clef Anthropic √† partir de .env et de lancer AIDER
      - AIDER a alors r√©pondu √† notre demande de modification et nous fournit ici la synth√®se de ses modifications
        - 1. Dans le fichier `2JSONV2.py` :
         - J'ai ajout√© un nouveau mode de test local (mode 3) dans la fonction `main()`.
         - J'ai cr√©√© des fonctions de remplacement (mock) pour `generate_image` et `text_to_speech` pour le mode de test local.
         - J'ai modifi√© la logique de s√©lection du mode pour inclure le nouveau mode de test.

        - 2. Dans le fichier `requirements.txt` :
         - J'ai sp√©cifi√© des versions pr√©cises pour chaque d√©pendance.
         - J'ai remplac√© 'dotenv' par 'python-dotenv'.
         - J'ai supprim√© 'peewee' car il ne semblait pas √™tre utilis√© dans le script actuel.

        - 3. Je n'ai pas modifi√© les fichiers `PLACE_HOLDER_TEXTE_VIDEO.txt` et `story.json`.

        Ces modifications visent √† am√©liorer la testabilit√© du script et √† assurer une meilleure reproductibilit√© de l'environnement de d√©veloppement. Le mode de test local permet de tester le flux de travail sans faire d'appels API r√©els, ce qui peut √™tre utile pour le d√©bogage et les tests rapides.

- [**R√©sultat final**](https://www.youtube.com/watch?v=1y_-vTEQ0Oo) 

9. **Text to CAD**
- ‚öôÔ∏è :Sonnet 3.5 semble √™tre capable de cr√©er un [programme python capable de g√©n√©rer des formes complexes](https://claude.ai/chat/91026ba9-f74b-4622-b215-3148ada38543)

- ‚öôÔ∏è :[**BeeGraphy**](https://www.youtube.com/watch?v=MdNp6nQgqgU) : Text to CAD via remote API. Tutorial en cours 

- ‚öôÔ∏è A voir le [tutorial](https://zoo.dev/docs/tutorials/text-to-cad) 



11. **Web scraping**:
  - ‚¨ú :[Les outils de web scraping 2024](https://youtu.be/od6AaKhKYmg?si=bol1exHiamBqdTmH)  :**Jina AI Reader API**, Mendable Firecrawl, and Crawl4AI and More"
  - [Storm](https://github.com/stanford-oval/storm): STORM est open source et muni de vision (GPT4-o) : il √©crit des articles comme Wikipedia √† partir de rien mais il s'informe via Internet. On peut le tester [ici üõ†Ô∏è](https://storm.genie.stanford.edu/)


Try out our live research preview to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!
  - 

                
- ‚¨ú:  [**beautifulsoup-vs-scrapy**: qui est le meilleur](https://scrapingrobot.com/blog/beautifulsoup-vs-scrapy/)
  

13. **Traitement d'images** :

   
- Les traitements possibles sont nombreux, c'est pourquoi un logiciel tout en un, accessible par API via Docker est interessant. Nous avons d√©j√† install√© Compfyui sur docker. Reste √† int√©rer le bon module:
- Trouver [controlnet-union-sdxl-1.0](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0) sur HuggingFace
- ‚öôÔ∏è: [Un tutorial n√©cessaire pour la prise en main de ComfyUi](https://www.youtube.com/watch?v=zpJQUw_0lBI)  




  
15. ‚öôÔ∏è [**Apprendre de fa√ßon active avec sonnet**](https://www.youtube.com/watch?v=Wf9hRb6pBXA)
16. ‚öôÔ∏è **RAG**:
- ‚¨ú [**local et infini**](https://www.youtube.com/watch?v=5xPvsMX2q2M)
- ‚¨ú  : [**Tutorial pour interagir avec les data en python**](https://github.com/projectwilsen/KnowledgeGraphLLM/tree/main/tutorial). [Video YouTube](https://youtu.be/ky8LQE-82xs?si=fBtFkNnRC5BsJHaj)
- ‚¨ú  : [**GraphRAG**](https://youtu.be/6Yu6JpLMWVo?si=i2pk2P7yP60Q0-LM)
- ‚¨ú  : [**GraphRAG en local avec Groq, recherches  locale et globale**](https://youtu.be/xkDGpR5g9D0?si=fNW3yN-VV9K0-BZF)
- ‚¨ú : [Agentic RAG dans colab](https://colab.research.google.com/drive/1-cTexfgPITZ6jkSLHsP7uObdfE1rfDxu?usp=sharing]  avec [une vid√©o explicative](https://www.youtube.com/watch?v=QQAkXHRJcZg&t=1s)
-  ‚úÖ : comparaison d'e l'analyse d'un document par [**sonnet**](https://claude.ai/chat/ad5d814e-8723-4d9d-b85e-f1da95ce1150) , [**perplexity**](https://www.perplexity.ai/search/dis-moi-ce-que-tu-penses-du-do-rOeCbqOfTqu90EG_HRlomQ) et e
      

18.   ‚úÖ **PC distant** : [Replicate](https://replicate.com/) : permet l'acc√®s √† des ressources (comme [la cr√©ation d'images](https://replicate.com/bytedance/sdxl-lightning-4step/api) via des API ou playground :essais gratuit puis co√ªt suivant le temps pass√©

20. ‚¨ú : A √©valuer [Invideo](https://invideo.io/i/samsonvowles), [Replicate](https://replicate.com/), [Live-portrait](https://replicate.com/fofr/ve-portrait), [Toon crafter]https://replicate.com/fofr/tooncrafter), [Oldify](https://replicate.com/arielreplicate/oldify), [Pixverse](https://app.pixverse.ai/),[Pika](https://pika.art/), [Filmora](https://filmora.wondershare.net), [anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main)


  

23. **Text to Graph**:
- ‚¨ú :  [Story teller avec bifurcation dans l'histoire possible](https://github.com/langchain-ai/story-writing)
- ‚¨ú :  [Un tutoriel pour chatter avec un  Knowledge Graph using LLMs](https://www.youtube.com/watch?v=KMXQ4SVLwmo)
- ‚¨ú :  [GraphFlow](https://github.com/infiniflow/ragflow?tab=readme-ov-file) : RAGFlow propose une extraction de connaissances √† partir de donn√©es non structur√©es et complexes, prend en charge le chunking intelligent, offre des citations ancr√©es avec une visualisation des r√©f√©rences, et supporte divers formats de donn√©es, y compris les graphiques. Il facilite un flux de travail RAG automatis√© et configurable, adapt√© √† toutes les entreprises, avec des API intuitives pour une int√©gration facile.
- Des logiciels comme Grasshopper et Flowise basent la r√©solution de probl√®mes complexes en les d√©composants et en les interconnectant sous la forme d'un graphe agen√ßant les briques repr√©sentatives des sous-probl√®mes √† traiter. Cela demande certains efforts √† l'utilisateur qui doit se souvenir des briques possibles, il doit aussi les interconnecter manuellement. Enfin le r√©sultat final est souvent difficilement lisible. On peut certes cr√©er des macro-briques pour faciliter la lisibilit√© au d√©triment de nouvelles interventions manuelles.

Un LLM de bon niveau peut quant'√† lui prendre en compte directement le besoin exprim√© par l'op√©rateur et g√©n√©rer automatiquement un graphe repr√©sentatif qu'il soumet √† l'op√©rateur. De plus le LLM peut modifier, expliquer et utiliser ce graphe en fonction du contexte et de la demande de l'op√©rateur. On a donc l'avantage d'une repr√©sentation visuelle efficace sans en payer le prix.

Nous avons valid√© cette approche √† travers un [premier dialogue avec Sonnet](https://claude.ai/chat/662358c3-056a-43d5-9fd0-11aee95b6a1a ) suivi d'un   [second dialogue](https://claude.ai/chat/ab1debd5-6406-45f9-9516-0d8d47461ab6) 


Le r√©sultat est tout √† fait correct : 
[graphe interactif](https://claude.site/artifacts/97fe422a-eb69-4c30-ad06-0caa1da58694)

<img src="https://github.com/user-attachments/assets/6c3278cd-34e6-4f3e-8224-4cada7a21f35" width=50%)>


- On peut m√™me faire des [svg interactif](https://claude.site/artifacts/1c2c5656-09c6-4e00-8e43-93284b0ebc6c)
- et [adapter la forme et la couleur des blocs](https://claude.site/artifacts/62a55f84-d683-4caf-852b-8a5c9443406f)
- Enfin le r√©sultat ainsi obtenu peut √™tre g√©n√©ralis√© et adapt√© √† des cas plus complexes [en sp√©cifiant les agents susceptibles d'y parvenir](https://claude.site/artifacts/3e3e1c14-d880-44b9-aa79-05bdcefa4bc0).
- L'utilisation d'agents n'est vraiement utile que si des actions "intelligentes" doivent √™tre entreprises, dans tous les autres cas il vaut mieux programmer (en no code !) la fonction que l'on souhaite r√©aliser. Avec de bonnes directives, Sonnets 3.5 produit un [r√©sultat tr√®s satisfaisant](https://claude.site/artifacts/05390ce0-4167-4330-abdb-eb2c8915cd75), m√™me si nous n'avons pas pouss√© plus loin le dialogue avec Sonnet pour √©laborer un meilleur graphe et un meilleur prompt syst√®me pour l'obtenir :
<img src="https://github.com/user-attachments/assets/e71b6e7a-f416-4368-bb8a-2345299cf8e9" width=50%>
 



  



**TO DO  ‚¨ú / DONE ‚úÖ** / **en cours** ‚öôÔ∏è / **tools** üõ†Ô∏è / **innovations** üí°
|        |        |  |
|-------------------|-------------------|-------------------|
| - Prompting (12) ‚öôÔ∏è| - Site web sur Github (7) ‚úÖ      | Traitement d'images(13)  ‚öôÔ∏è |
| - üõ†Ô∏è Docker  ‚úÖ| - Simple API locale (1) ‚úÖ      | Speech to text  ‚¨ú (30)|
| - üõ†Ô∏è[Github](https://github.com/) (10) ‚úÖ| -  (5)Programmation no code (AIDER ‚úÖ), (MicroAgent ‚¨ú)     |- Web scrapping via python (11) ‚öôÔ∏è      |
| - üõ†Ô∏è GPT-4o ‚úÖ et [playground](https://platform.openai.com/playground/chat?models=gpt-4o)| -  üõ†Ô∏è LM Studio ‚úÖ       |- RAG  (16)  ‚öôÔ∏è     |
| - üõ†Ô∏è Anthropic [chat](https://claude.ai/new) & [API](https://www.anthropic.com/api-bk) (Sonnet 3.5) ‚úÖ| - Text to speech via python: payant(3. ‚úÖ) et gratuit (19. ‚¨ú)     |-  GPTs  ‚¨ú  |
| - üõ†Ô∏è[Perplexity](https://www.perplexity.ai/)   ‚úÖ|- Text to vid√©o et traitements (3)    ‚öôÔ∏è       |-  Agents (21) ‚¨ú  |
| - üõ†Ô∏è Comfyui   ‚úÖ| - ‚¨ú:  Tutoriel video automatique     |- Function calling (Gorilla) ‚¨ú   |
| - üõ†Ô∏è Copilot ‚úÖ| -  Cr√©ation d'images consistantes  ‚úÖ    |- Text to CAD (9) ‚öôÔ∏è      |
| - üõ†Ô∏è Anaconda ‚úÖ| - Cr√©ation de tutoriel vid√©o  ‚úÖ       |- Serveur local ‚¨ú    |
| - üõ†Ô∏è[Hedra](https://www.hedra.com/) ‚úÖ | - Vid√©o-livre narratif g√©n√©r√© √† partir de texte (8) ‚úÖ      |- Remote PC (18) ‚úÖ   |
| - üõ†Ô∏èMistral| - LLM via python (2)  ‚úÖ      |-  [LLM en //](https://youtu.be/6hG39mr9c0k?si=jwaLEMEDurlWbHuF)   ‚¨ú  |
| - üõ†Ô∏è[Deepseek api](https://www.deepseek.com/) ‚¨ú  | - Text to image local (6)   ‚úÖ     |-  Open interpreter ‚¨ú  |
|- üõ†Ô∏è [Groq](https://groq.com/) ‚úÖ |üõ†Ô∏è Hugging Face ‚úÖ  |- Coder √† partir de GitHub, HF, papers with code  ‚¨ú    |
|- üõ†Ô∏èVS studio ‚úÖ |- üõ†Ô∏è Kaggle ‚úÖ   |- üõ†Ô∏è Colab  ‚úÖ  |
|- Caract√®res consistants (14) ‚öôÔ∏è |-  Apprendre de fa√ßon active (15) ‚öôÔ∏è  |- ‚öôÔ∏è D√©tection automatique avec Sonnet (17)  |
|- üõ†Ô∏è [Poe](https://poe.com/) ‚úÖ |-  üõ†Ô∏è Web Apps by [123apps](https://123apps.com/) :Edit, Convert, Create |- üõ†Ô∏è [VAPI](https://docs.vapi.ai/introduction) : your remote phone assistant |
|- ‚¨ú : üõ†Ô∏è √† √©valuer (20)  |-‚¨ú : Fabrication low cost (26)  |- ‚¨ú : Avatar parlant ou chantant gratuit (28)  |
|- ‚öôÔ∏è: LearnAnything (22) üí° | - ‚öôÔ∏è: Text to Graph (23) üí°   |- ‚¨ú : RAG to Graph     |
|- ‚öôÔ∏è : API de calcul scientifique (24) üí° |- ‚¨ú : D√©ploiement du chatbot (25)      | Bot assistant multimodal (27)   |
|- ‚¨ú : [OpenedAI Vision (~ GPT4-vision)](https://www.youtube.com/watch?v=61F_4zfs_Jg)|- ‚¨ú : Traitement images et video (31)   |-  |
| - ‚¨ú : Automatisation de t√¢ches (32) | Am√©lioration des performances d'un LLM (33)  |  |



## Les incontournables utilisables sans rien automatiser. 

- [Mieux vaut regarder les benchmarks pour choisir](https://klu.ai/glossary/mmlu-pro-eval)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/57f69c6d-9505-4b23-82a7-eee9025e392e" width="600" >

  - ‚¨ú :
  
    - [QWEN2](https://qwenlm.github.io/blog/qwen2) est semble-t-il tr√®s performant (128 k de contexte)

    -  [gemma2](https://artificialanalysis.ai/models/gemma-2-9b)
      
    -  ![image](https://github.com/user-attachments/assets/7e4a4f2d-373b-437e-82dd-dcfc22d4df21)

    -  Mise en oeuvre des mod√®les locaux avec [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE) : Ultra-rapide, d√©velopp√© en Rust  

- Pour les maths et le raisonnement en local : [mathstral](https://mistral.ai/news/mathstral/)
![image](https://github.com/user-attachments/assets/50f68eed-55ce-411c-8878-31c09fbcd57a)

- Codestral Mamba, de Mistral AI, est un mod√®le innovant qui peut traiter les s√©quences rapidement, quelle que soit leur longueur, capable de g√©rer jusqu'√† 256k tokens avec des performances √©quivalentes aux mod√®les transformateurs de pointe, particuli√®rement efficace pour les applications de productivit√© de code
<img src="https://github.com/user-attachments/assets/146d165a-645a-453b-b9d5-f2f671c3c2f6" width="70%">

- Donc utiliser **Sonnet 3.5** et **GPT-4o** pour avoir les meilleures r√©ponses √† nos questions.
- [**Perplexity**](https://www.perplexity.ai/) est un autre incontournable pour surfer sur le web ( que nous contournerons quand m√™me plus tard ! üòä)
- [**Harpa**](Harpa.ai) permet d'interagir avec une page web ou une video YouTube
- **Copilot de Microsoft**
- Je vous laisse le soin de tester ces diff√©rents logiciels qui m√™me dans leurs versions gratuites am√©lioreront sensiblement votre productivit√©.


 
 ## **Learn AnyThing**:
- "Learn anything" est capable de partir d'un besoin et se transforme en v√©ritable professeur attitr√©:
- Comment y parvenir: 
  - L'interaction avec l'utilisateur pour cibler pr√©cis√©ment son besoin (passer de "apprendre l'IA" √† "produire des vid√©os de fa√ßon automatique" par exemple. Pour cela il va nnous falloir apprendrele "_prompting_",
  - La collecte de donn√©es, le LLM a une connaissance g√©n√©rale pas tr√®s pointue ni tr√®s √† jour: pour y rem√©dier il faut aller chercher les donn√©es sur internet, c'est le _web scraping_
  - Ces donn√©es, il faut que le LLM soit capable de les stocker et de s'en servir en fonction du contexte, c'est le _Retrieval-Augmented Generation (RAG)_
  - Le LLM est devenu expert de son sujet , √† charge pour lui de transmettre son savoir
    - Un cours attrayant d'abord, pour cela il va devoir cr√©er une video, c'est le _video_making_
    - Un outil pour tracer la progression de son √©l√®ve, c'est la [_palette de progression_](https://claude.ai/chat/f1134fe8-632f-4590-89cb-6560f41acf8c) Cela nous a aussi permis de tester la richesse d'un travail en √©quipe avec l'IA :l'am√©lioration est spectaculaire:
<img src="https://github.com/user-attachments/assets/cf7c1b19-bc1f-41d6-9256-da5d6a013311" width="50%"> 
   - Un outil d'interface avec l'utilisateur qui montre son √©tat d'avancement dans l'acquisition de ce nouveau savoir
     - Sonnet nous offre un hyperlien - https://claude.site/artifacts/36f2d592-7b03-4023-b877-5c1b7b5f440f - vers un HTML interactif avec l'√©l√®ve qui peut ainsi naviguer dans les savoir √† acqu√©rir tout en visualisant sa progression

      
- de donn√©es obtenues par web scraping ou de donn√©es locales multimodales. Les images peuvent √™tre interpr√©t√©es, et les donn√©es graphiques et tableaux sont r√©cup√©r√©s et int√©gr√©s dans les graphes de connaissance. Cette fonctionnalit√© permet d'enrichir les graphes avec des informations actualis√©es et diversifi√©es, provenant de diff√©rentes sources.
- La connaissance peut √™tre organis√©e de mani√®re efficace sous forme de graphe, comme illustr√© dans l'image ci-dessous :

<img src="https://github.com/user-attachments/assets/9c5d7cdd-b4e8-48ed-8a33-c2f8059b17e8" width= "50%">

- Un mod√®le de langage large (LLM) est capable de g√©n√©rer des graphes interactifs (voir r√©f√©rence 23.) √† partir d'informations m√™me d√©structur√©es.
- Pour un utilisateur de la plateforme "Learn anything", chaque n≈ìud repr√©sente une connaissance √† acqu√©rir. Le LLM (ou un agent sp√©cifique) peut mettre en ≈ìuvre plusieurs fonctionnalit√©s pour aider √† cet apprentissage :
    - _Faire un cours_ : Cr√©er des cours, y compris des vid√©os automatiques (voir r√©f√©rence 3.), qui peuvent s'adapter √† la langue et au niveau de l'utilisateur.
    - _Poser des questions_ : Encourager un apprentissage actif en posant des questions dont la complexit√© s'ajuste au niveau de l'utilisateur.
    - _Proposer des solutions_ : Offrir des solutions, des corrections adapt√©es √† la r√©ponse re√ßue ou des pistes de r√©solution.
    - _Suivi du niveau de connaissance_ : Maintenir un √©tat du niveau de connaissance acquis, en changeant par exemple la couleur du n≈ìud de blanc √† vert pour les connaissances ma√Ætris√©es.
    - _S√©quen√ßage des r√©activations_ : Activer des s√©quences de r√©activation de la connaissance dont la fr√©quence diminue √† mesure que l'acquisition progresse.
    - _Multimodalit√©_ : Utiliser la voix, l'image et la vid√©o √† la fois en entr√©e et en sortie pour une efficacit√© accrue.
    - _Personnalisation avanc√©e_ : Impl√©menter des algorithmes de personnalisation plus sophistiqu√©s pour s'adapter aux pr√©f√©rences et au style d'apprentissage de chaque utilisateur.
    - _Analyse des donn√©es d'apprentissage_ : Utiliser des analyses de donn√©es pour identifier les points faibles et proposer des plans d'am√©lioration personnalis√©s.
    - _Gamification_ : Introduire des √©l√©ments de gamification pour rendre l'apprentissage plus engageant et motivant.
    - _Feedback en temps r√©el_ : Fournir des retours en temps r√©el sur les performances de l'utilisateur pour un apprentissage plus r√©actif.
- Voir la fin du [dialogue avec Perplexity](https://www.perplexity.ai/search/what-is-the-interest-of-llama-6lm_fKVKQqeLarBCK0Z30g) qui nous a orient√© sur l'emploi de LangChain dau d√©pend de notre id√©e initiale  (_llama-index agent_). 
- Nous allons tenter le no code avec LangGraph pour r√©pondre √† notre besoin dans un nouveau [dialogue avec Sonnet 3.5](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793). Malheureusement il donne un code non coh√©rent avec la r√©alit√© de LangGraph car il refuse de faire des recherches sur le net. Nous nous sommes donc report√© sur [Perplexity](https://www.perplexity.ai/search/detaille-langgraph-promptnode-PeOQJBUAT9mAJRrp.vjg5g).
- Nous revenons sur [sonnet 3.5 avec le code propos√© par Perplexity](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793) pour g√©n√©rer le code dont voici la structure 
  <img src="https://github.com/user-attachments/assets/2fa4a265-ce1d-4086-b1c6-e89a079ad8f0" width="50%">
- Il ne ns restera plus qu'√† valider le code avec l'aide d'AIDER. Nous utilisons l'environnement conda "learnanything" qui permet le lancement de aider en tapant simplement dans un terminal `aider`
- Nous avons lanc√© les divers tests avec l'aide d'aider qui proposait des changements si n√©cessaire. [Sonnet 3.5 fait la synth√®se](https://claude.ai/chat/c2ad00e6-187f-4bae-aae5-7e13b2b3ac7a) 
- Ma conclusion est que l'activation d'agents est possible, mais tr√®s lente et relativement co√ªteuse. Nous avons probablement aussi exag√©r√© sur le nombre d'agents √† mettre en oeuvre. Il faut mieux que le traitement de la requ^te initiale soit priseen compte par un seul LLM  qui , avec le bon prompt peut produire le graphe ainsi que l'artefact qui permet sa visualisation. Le LLM ma√Ætrise la coh√©rence d'ensemble et peut donc facilement adapter le r√©sultat √† des adaptations souhait√©e par l'utilisateur. 
        

# Les probl√®mes que nous solvons ... ou pas pas √† pas 
**TO DO  ‚¨ú / DONE ‚úÖ** / **en cours** ‚öôÔ∏è / **tools** üõ†Ô∏è / **innovations** üí°
- **Avoir un bon environnement de programation no code**
  - D√©butant : sonnet 3.5 version pro avec Artefact est le plus facile √† utiliser : "Montre moi en HTML/CSS comment faire ...."
  - Interm√©diaire :
    - Evaluation de [gpt-engineer](gpte <project_dir>) . Nous avons suivi les instruction mais il faut installer Poetry et relancer l'ordinateur pour que les variables d'environnement prennent effet. **Son emploi n'est pas probant**
    - **AIDER:** son emploi est facile, dans le r√©pertoire de travail (il va cr√©er un .git s'il n'existe pas) et l'environnement conda "teambot" on l'active avec la commande AIDER. Il faut mettre la variable d'environnement :`$env:ANTHROPIC_API_KEY = "sk...`
- Run aider with the files you want to edit: `aider <file1> <file2>`
| **Commandes principales d'Aider** | **Fonctionnalit√©s cl√©s** |
| --- | --- |
| **/add** [nom_fichier] : Ajouter des fichiers √† la session de chat pour que l'IA puisse les modifier ou les examiner | Int√©gration Git automatique avec commits intelligents |
| **/clear** : Effacer tout l'historique de chat pr√©c√©dent | Prise en charge de plusieurs langages de programmation populaires |
| **/commit** [message] : Enregistrer les modifications dans le d√©p√¥t avec un message de commit | Capacit√© √† √©diter plusieurs fichiers simultan√©ment |
| **/diff** : Afficher les diff√©rences entre les versions des fichiers | Utilisation d'une carte du d√©p√¥t Git entier pour une meilleure compr√©hension du contexte |
| **/drop** [nom_fichier] : Retirer des fichiers de la session en cours | Possibilit√© d'ajouter des images et des URLs au chat |
| **/exit** ou **/quit** : Fermer l'application Aider | Codage par commande vocale |
| **/git** [commande] : Ex√©cuter des commandes Git | |
| **/help** : Afficher les informations sur les commandes disponibles | Bonnes pratiques |
| **/lint** [nom_fichier] : V√©rifier et corriger les erreurs de code | Ajoutez les fichiers pertinents au contexte du chat avec /add |
| **/ls** : Lister tous les fichiers connus et leur statut dans le chat | Effectuez des changements petits et it√©ratifs plut√¥t que de grandes requ√™tes |
| **/model** [nom] : Passer √† un mod√®le de langage diff√©rent | Utilisez /run pour ex√©cuter des tests et fournir un retour √† Aider |
| **/models** : Rechercher parmi les mod√®les de langage disponibles | Tirez parti de /lint pour les corrections et am√©liorations automatiques du code |
| **/run** [commande] : Ex√©cuter des commandes shell | Utilisez /diff pour examiner les changements avant de les valider |
| **/test** : Ex√©cuter des commandes shell et enregistrer les erreurs | Exp√©rimentez avec diff√©rents mod√®les en utilisant /model pour des r√©sultats optimaux |
| **/tokens** : Rapporter le nombre de tokens utilis√©s dans le chat | |
| **/undo** : Annuler le dernier commit Git effectu√© par Aider | |
| **/voice** : Enregistrer et transcrire l'entr√©e vocale | |
| **/web [url]** : Extraire le contenu d'une page web avec Selenium sans interface graphique | |
<br><br>
<img src="https://github.com/user-attachments/assets/def6af11-570c-4f83-96b3-25813f13b437" width="70%">
<br><br>











