# [SimpleAIRoadMap](https://chatgpt.com/share/d2c569e9-b863-4143-959c-583dc10b6257) *:
*: Nous avons mis un hyperlien sur le chat d'aide à la production de ce markdown 

# **How to master AI step by step from beginner to expert level**

_**Savoir Lire, Ecrire , Compter : A l'heure de l'IA, ses savoir-faire de base se sont étendus:**_

- **Lire** c'est aussi naviguer sur le web (_web scraping_) et apprendre en regardant des vidéos sur YouTube (_video reading_) 
- **Ecrire** c'est aussi chatter (_prompting_) avec l'IA (_LLM_) , envoyer des mails(_automation_), créer des vidéos (video_making_) ou utiliser un espace de stockage qui nous est propre et facilement accessible (_RAG_). C'est aussi savoir communiquer avec notre environnement (_multimodal_)
- **Compter** c'est aussi lancer des programmes (_coding_), accéder à des ressources externes(_API)_

L'IA doit aussi savoir s'adapter au niveau de l'utilisateur :

- Pour le __débutant__, elle fournira des "boîtes noires" qui réaliseront certaines des tâches décrites plus haut sans que l'utilisateur ait à faire d'effort. C'est le _"What does it do"_.
- Pour l'__apprenti__, elle permettra de réaliser des tâches plus complexes en associant les outils disponibles. C'est le _"How to use it"_.
- Pour celui qui __maîtrise__, il adaptera les boîtes existantes à ses besoins. C'est le _"How to improve what it does"_.
- Pour l'__expert__ enfin, il créera ses propres outils. C'est le _"How to create new tools"_.

Si toutes ces capacités sont intégrés dans notre IA, elle devient un parfait assistant 
D'autres contraintes doivent être intégrer durant notre parcours: Souhaite-t-on des solutions locales ou déportées,payantes ou gratuites ?

- [**Classification des possibilitées de l'IA**](https://chatgpt.com/share/c7d34658-7c48-4e70-ad22-01c3d5071553) 

  - **Data Acquisition** :Text/Voice/Image/Video/Web Scraping/External Tool API Access
  - **Data Processing** : Prompting/RAG/LLM/mLLM/Reasoning/Automation/Tools creation 
  - **Data and Actions Creation** :Text/Voice/Image/Animation/Video/3D modeling/API Endpoint Development/Virtual Operator/Development Environment
  - **All in one** : Agents/Coding assistant/Data scraping assistant/Teaching Assistant/Project assistant/Expert assistant

<br>
**TO DO  ⬜ / DONE ✅** / **en cours** ⚙️ / **tools** 🛠️ / **innovations** 💡
<br>

| Data Acquisition            | Débutant | Intermédiaire | Maîtrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Text                        |🛠️ [NotebookLM](https://notebooklm.google/),<br>🛠️ [Perplexity](https://www.perplexity.ai)         |               |          |        |
| Voice                       |          |               |          |        |
| Image                       |          |               |          |        |
| Video /Vision               |          |               | [openedai-vision](https://github.com/matatonic/openedai-vision)         |        |
| Web Scraping                |🛠️ [Perplexity](https://www.perplexity.ai) <br>🛠️ [Harpa](Harpa.ai)          |               |          |        |
| External Tool API Access    |          |               |          |        |
<br>
# Table des Matières

- [Introduction](#introduction)
- [Détails Techniques](#détails-techniques)
- [Conclusion](#conclusion)

<br>

| Data Processing             | Débutant | Intermédiaire | Maîtrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| [Prompting](https://chatgpt.com/share/1bc7eb5b-5afd-4d57-a963-5fc3b73ae1fa)                   | [Metaprompt](https://chatgpt.com/share/9bf5e118-0943-4d83-98fa-40b4ade83247)          |               |          |        |
| RAG (Retrieval-Augmented Generation) |          |               |          |        |
| LLM (Large Language Models) |[Sonnet Chat](https://claude.ai/new)<br>[ChatGPT](https://chatgpt.com/)          | [OpenAI playground & API](https://platform.openai.com)  <br> [Deepseek api](https://www.deepseek.com/) <br> [Groq](https://groq.com/)  <br> [Mistral](https://mistral.ai/) <br> [Sonnet API](https://www.anthropic.com/api-bk)          |  [mathstral](https://mistral.ai/news/mathstral/)        |        |
| LLM Multimodal |          |               |          |        |
| Reasoning                   |          |               |          |        |
| Automation                  |          |               |          |        |
| Tools creation              |          |               |          |        |

<br>


| Data and Actions Creation   | Débutant | Intermédiaire | Maîtrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Text                        |          |               |          |        |
| Voice                       |          |               |          |        |
| Image                       |          |               |          |        |
| Animation                   | [hedra](https://www.hedra.com/)         |               |          |        |
| Video                       |          |               |          |        |
| Web app                     |          |               |          |        |
| 3D modeling                 |          |               |          |        |
| API Endpoint Development    |          |  [Gorilla](https://gorilla.cs.berkeley.edu/)              |          |        |
| Virtual Operator            |          |               |          |        |
| Local Development Environment      |[Github](https://github.com/) <br> [LMStudio](https://lmstudio.ai/)         |  [Lightning AI](https://chatgpt.com/share/05b67f08-a0da-495c-a295-198e63e6c9bd) <br> [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE)  <br> [Docker](https://www.docker.com/)            |          |        |
| Cloud based Development Environment            | [[colab.google](https://www.perplexity.ai/search/quelles-sont-les-principales-c-NcNfvE5bRASAqlxa6IoQQA) <br> [[kaggle.com](https://www.perplexity.ai/search/quelles-sont-les-principales-c-eZGY4KOjRyOxAXcPjfMKVA) <br> [huggingface.co](https://www.perplexity.ai/search/https-huggingface-co-quelles-s-45BlLQtASYGCMo4QEsGixA) <br>         |               |          |        |

<br>

| All in One                  | Débutant | Intermédiaire | Maîtrise | Expert |
|-----------------------------|----------|---------------|----------|--------|
| Agents                      |          |               |          |        |
| Chatbot          |          |  [Poe.com](https://www.perplexity.ai/search/chatbot-https-poe-com-principa-xeo_mfVZRZSmg.MJYxnibA)        |          |        |
| Coding assistant            |          | [aider.chat](https://www.perplexity.ai/search/aider-code-assistant-cheat-she-REvsq3zxSwS75PPD_HcVeQ)              |          |        |
| Data scraping assistant     |          |               |          |        |
| Teaching Assistant          |          |               |          |        |
| Project assistant           |          |               |          |        |
| Remote phone assistant      |          |               | [VAPI](https://docs.vapi.ai/introduction)         |        |
| Expert assistant            |          |               |          |        |
<br>
  





__***Avec le monde d'avant l'IA on apprenait à faire puis on faisait. Avec le monde d'aujourd'hui on fait faire à l'IA, puis on apprend à partir de ce que l'IA a fait, on finit donc par savoir faire aussi. Du coup plus besoin de prof, il ne suffit que de vouloir faire pour parvenir à nos fins !! The sky is the limit !!!***__

**TO DO  ⬜ / DONE ✅** / **en cours** ⚙️ / **tools** 🛠️ / **innovations** 💡
|        |        |  |
|-------------------|-------------------|-------------------|
| - Prompting (12) ⚙️| - Site web sur Github (7) ✅      | Traitement d'images(13)  ⚙️ |
| - 🛠️ Docker  ✅| - Simple API locale (1) ✅      | Speech to text  ⬜ (30)|
| - 🛠️[Github](https://github.com/) (10) ✅| -  (5)Programmation no code (AIDER ✅), (MicroAgent ⬜)     |- Web scrapping via python (11) ⚙️      |
| - 🛠️ GPT-4o ✅ et [playground](https://platform.openai.com/playground/chat?models=gpt-4o)| -  🛠️ LM Studio ✅       |- RAG  (16)  ⚙️     |
| - 🛠️ Anthropic [chat](https://claude.ai/new) & [API](https://www.anthropic.com/api-bk) (Sonnet 3.5) ✅| - Text to speech via python: payant(3. ✅) et gratuit (19. ⬜)     |-  GPTs  ⬜  |
| - 🛠️[Perplexity](https://www.perplexity.ai/)   ✅|- Text to vidéo et traitements (3)    ⚙️       |-  Agents (21) ⬜  |
| - 🛠️ Comfyui   ✅| - ⬜:  Tutoriel video automatique     |- Function calling (Gorilla) ⬜   |
| - 🛠️ Copilot ✅| -  Création d'images consistantes  ✅    |- Text to CAD (9) ⚙️      |
| - 🛠️ Anaconda ✅| - Création de tutoriel vidéo  ✅       |- Serveur local ⬜    |
| - 🛠️[Hedra](https://www.hedra.com/) ✅ | - Vidéo-livre narratif généré à partir de texte (8) ✅      |- Remote PC (18) ✅   |
| - 🛠️Mistral| - LLM via python (2)  ✅      |-  [LLM en //](https://youtu.be/6hG39mr9c0k?si=jwaLEMEDurlWbHuF)   ⬜  |
| - 🛠️[Deepseek api](https://www.deepseek.com/) ⬜  | - Text to image local (6)   ✅     |-  Open interpreter ⬜  |
|- 🛠️ [Groq](https://groq.com/) ✅ |🛠️ Hugging Face ✅  |- Coder à partir de GitHub, HF, papers with code  ⬜    |
|- 🛠️VS studio ✅ |- 🛠️ Kaggle ✅   |- 🛠️ Colab  ✅  |
|- Caractères consistants (14) ⚙️ |-  Apprendre de façon active (15) ⚙️  |- ⚙️ Détection automatique avec Sonnet (17)  |
|- 🛠️ [Poe](https://poe.com/) ✅ |-  🛠️ Web Apps by [123apps](https://123apps.com/) :Edit, Convert, Create |- 🛠️ [VAPI](https://docs.vapi.ai/introduction) : your remote phone assistant |
|- ⬜ : 🛠️ à évaluer (20)  |-⬜ : Fabrication low cost (26)  |- ⬜ : Avatar parlant ou chantant gratuit (28)  |
|- ⚙️: LearnAnything (22) 💡 | - ⚙️: Text to Graph (23) 💡   |- ⬜ : RAG to Graph     |
|- ⚙️ : API de calcul scientifique (24) 💡 |- ⬜ : Déploiement du chatbot (25)      | Bot assistant multimodal (27)   |
|- ⬜ : [OpenedAI Vision (~ GPT4-vision)](https://www.youtube.com/watch?v=61F_4zfs_Jg)|- ⬜ : Traitement images et video (31)   |-  |
| - ⬜ : Automatisation de tâches (32) | Amélioration des performances d'un LLM (33)  |  |



# AI-automation
Tout faire avec l'IA. Elle fait le boulot sous votre contrôle et vous forme à comprendre comment tout ça fonctionne.

L'idée est de construire pas à pas une "baquette magique" apte à tout faire. Nous nous bornerons à vous indiquer:
- Les bons outils à utiliser,
- Comment bien poser votre problème
- Comment apprendre cette nouvelle façon de travailler , en comprenant comment la magie opére.
- Cela passe par l'emploi d'outils pour :
  - Ne pas réinventer la roue, donc voir si notre problème n'est pas déjà résolu. c'est le "**web scraping**" en particulier sur Github, Hugging Face ou Papers with Code
  - Récupérer l'état de l'art c'est le "web scraping" de site cpomme Arxiv en particulier et le **RAG** pour stocker le savoir
  - Le programmer en "no code" avec les **coding assistants**
  - Utiliser des **agents** collaborant entre eux pour résoudre les problèmes les plus complexes 



## Les incontournables utilisables sans rien automatiser. 

- [Mieux vaut regarder les benchmarks pour choisir](https://klu.ai/glossary/mmlu-pro-eval)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/57f69c6d-9505-4b23-82a7-eee9025e392e" width="600" >

  - ⬜ :
  
    - [QWEN2](https://qwenlm.github.io/blog/qwen2) est semble-t-il très performant (128 k de contexte)

    -  [gemma2](https://artificialanalysis.ai/models/gemma-2-9b)
      
    -  ![image](https://github.com/user-attachments/assets/7e4a4f2d-373b-437e-82dd-dcfc22d4df21)

    -  Mise en oeuvre des modèles locaux avec [HuggingFace candle](https://www.youtube.com/watch?v=8hpYedvjrxE) : Ultra-rapide, développé en Rust  

- Pour les maths et le raisonnement en local : [mathstral](https://mistral.ai/news/mathstral/)
![image](https://github.com/user-attachments/assets/50f68eed-55ce-411c-8878-31c09fbcd57a)

- Codestral Mamba, de Mistral AI, est un modèle innovant qui peut traiter les séquences rapidement, quelle que soit leur longueur, capable de gérer jusqu'à 256k tokens avec des performances équivalentes aux modèles transformateurs de pointe, particulièrement efficace pour les applications de productivité de code
<img src="https://github.com/user-attachments/assets/146d165a-645a-453b-b9d5-f2f671c3c2f6" width="70%">

- Donc utiliser **Sonnet 3.5** et **GPT-4o** pour avoir les meilleures réponses à nos questions.
- [**Perplexity**](https://www.perplexity.ai/) est un autre incontournable pour surfer sur le web ( que nous contournerons quand même plus tard ! 😊)
- [**Harpa**](Harpa.ai) permet d'interagir avec une page web ou une video YouTube
- **Copilot de Microsoft**
- Je vous laisse le soin de tester ces différents logiciels qui même dans leurs versions gratuites amélioreront sensiblement votre productivité.

## Les incontournables pour automatiser:
- Les mêmes (ou leur équivalent) accessibles à l'intérieur d'un code Python. 
- Des outils produisant et mettant en oeuvre le code à notre place
- Des outils pour produire automatiquement des vidéos qui nous servirons à apprendre ce que fait l'IA
- Des outils pour [appeler des fonctions externes](https://gorilla.cs.berkeley.edu/leaderboard.html)
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/084708bc-b8f1-469e-94bd-32d48cc6cf50" width="600" />

## Architecture générale:
<img src="https://github.com/jpbrasile/AI-automation/assets/8331027/17a47369-f026-46b6-a11c-0dc0d48f35de" width="600" />

- Le coeur du système, les **LLM** (Large language model)  recoivent du texte, le traite et fournissent du texte en retour. Le texte d'entrée doit être tel qu'il exprime clairement et concrètement nos attentes (c'est le prompting)
- Le texte en retour peut être formaté pour correspondre à une réponse de type texte brut, JSON,  markdown, HTML , code , API ... suivant le post processing envisagé.

1. ## Création automatique d'une API web qui peut effectuer deux opérations mathématiques :

- Additionner deux nombres
- Multiplier deux nombres

- L'application doit être conteneurisée avec Docker pour faciliter son déploiement et son exécution.
- Vous voulez un guide étape par étape pour créer cette application, en partant de zéro, sans aucun outil préinstallé sur votre ordinateur.
- L'objectif final est de pouvoir non seulement créer cette application, mais aussi de pouvoir la partager facilement. 
- Vous voulez que n'importe qui puisse la télécharger et la lancer sur son propre ordinateur, quelle que soit sa configuration.
- Vous avez besoin d'instructions claires sur comment lancer l'application et comment la reproduire sur un autre PC.
- [**Dialogue avec sonnet 3.5** pour mettre en oeuvre la solution en "manuel"](https://claude.ai/chat/a71daeb6-5875-4ecb-9dc6-7dce126afde0) 
- Nous verrons plus tard comment automatiser la mise en place de ce type d'application en automatique avec AIDER
  
## Faisons le tutoriel correspondant sous forme de vidéo
- L'idée est de partir de la synthèse récapitulée par sonnet 3.5 de notre programme précédent pour en faire un tuto.
- Pour cela on établit un [dialogue avec sonnet 3.5](https://claude.ai/chat/08fb3cc8-5cb4-45ed-9132-953e30ecf792) pour dégrossir le problème:
    - Pour créer les planches HTML support,
    - Le texte des voice over,
    - Le prompt pour produire les images
    - Le code python pour stocker ces données dans des répertoires
    - Le code python créant les MP3 et les images et qui les stockent
    - Le code python qui fait l'assemblage
- Ce dégrossissage montre qu'il est préférable d'avancer pas à pas en construisant et validant pas à pas le code python correspondant, ce que nous allons faire maintenant avec un nouveau thread sonnet 3.5.
2. **Créer un code python permettant de dialoguer avec sonnet 3.5**
  - On utilisera Visual Studio Code pour la mise en oeuvre et pour tester les codes
  - On utilisera anaconda pour créer un environnement logiciel spécifique. Nous utiliserons l'environnement teambot déjà créer avec `conda activate teambot` dans un terminal 
  - On crée un répertoire de travail video-maker dans lequel on met le fichier .env avec nos clefs API, ainsi que les fichiers requirements.txt et anthropic-api-hello-world.py créer par sonnet 3.5
  - Le dialogue fonctionne :
    
```bash
(base) PS C:\Users\test\Documents\AI_Automation\video_maker> conda activate teambot

(teambot) PS C:\Users\test\Documents\AI_Automation\video_maker> python anthropic-api-hello-world.py
Claude dit: [TextBlock(text='Bonjour !', type='text')]
```

3. [**Création d'une vidéo à partir d'un texte**](https://claude.ai/chat/c33dece9-e5ab-4206-98c6-de644cb1d731)  
- Ce projet automatise la création de vidéos éducatives à partir de contenu textuel, utilisant diverses technologies et APIs. Le processus se déroule en plusieurs étapes intégrées dans un script Python unique :
  - Conversion du texte :
    - Lit le contenu du fichier PLACE_HOLDER_TEXTE_VIDEO.txt.
    - Utilise l'API Claude d'Anthropic pour convertir le texte en structure JSON de diapositives.
  - Traitement des diapositives :
    - Génère un fichier HTML structuré avec CSS intégré pour chaque diapositive.
    - Crée un texte de voix off avec Claude.
    - Produit une image illustrative via l'API DALL-E d'OpenAI.
    - Génère un fichier audio de la voix off avec l'API Text-to-Speech d'OpenAI.
  - Création des vidéos :
    - Capture une image du HTML rendu avec Selenium.
    - Combine l'image et l'audio en utilisant MoviePy pour chaque diapositive.
  - Agrégation finale :
    - Assemble toutes les vidéos individuelles en une seule vidéo.
    - Ajoute des transitions entre les diapositives.
  - Le projet utilise Python avec diverses bibliothèques (BeautifulSoup, Requests, Pillow, MoviePy) et APIs (Anthropic, OpenAI). Cette approche intégrée offre une solution complète et efficace pour la production automatisée de contenu vidéo éducatif, de la conversion du texte à la création de la vidéo finale.
⚙️ : [**Text to video de longue durée**](https://video-infinity.tanzhenxiong.com/) en open source
- [Rajout d'effet spéciaux sonores](https://github.com/open-mmlab/FoleyCrafter):FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds
-  ⬜ : [Echange de visages dan une vidéo](https://www.youtube.com/watch?v=Iy0PaElchlg)
4. **Point d'étape:**
- Nous avons réussi à mettre en oeuvre une applicatoin complexe sans coder une seule ligne. Cependant ce faisant nous avons détecté des pistes pour augmenter encore notre productivité
  - **Automatiser les itérations de débuggage** , ce qui nous a fait perdre le plus de temps dans la mise au point du code
  - Passer à l'open source, en particulier pour la création d'image qui constitue le poste de dépense le plus élevé pour la création d'une vidéo
  - Faire du web scraping pour voir si notre problème n'est pas déjà résolu par ailleurs
 
5. **Coding assistant**:
- ✅: AIDER+Sonnet**
  - On crée le répertoire ```coding_assistant``` et on lance ```conda activate teambot```
  - On suit les [instructions d'installation](https://github.com/paul-gauthier/aider) 
  - Mais il faut l'adapter au terminal powershell :```$env:ANTHROPIC_API_KEY="sk... "```
  - AIDER répond à nos directives et adapte en conséquence un repository qui a été cloné localement.
  - Il conserve un logbook des actions entreprises (```.aider.chat.history.md```) et un LLM comme Sonnet 3.5 ou GPT-4o peut alors en faire la synthèse:
    ```J'ai eu des problèmes qui ont été résolu dans le document joint, fais en la synthèse```
  - AIDER est [SOTA](https://aider.chat/docs/leaderboards/) avec Sonnet 3.5 mais il est aussi très performant avec DeepSeekCoder
  - ⬜ :[AIDER avec deepSeek](https://youtu.be/Y-_0VkMUiPc?si=zAPZQrYj6yrMHhne)
-  ⬜ [**Micro-agent**](https://github.com/BuilderIO/micro-agent) : itération automatique sur des cas tests
-  ⬜ [**CodeGeeX4**](https://github.com/THUDM/CodeGeeX4) : en plugin de VS code

- 
6. [**Text to image local dans docker**](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
- Il faut "alimenter" comfyui en y rajoutant les chkpoints requis à placer dans le répertoire : ```C:\Users\test\Documents\AI_Automation\coding_assistant\comfyui\storage\ComfyUI\models\checkpoints```
- Il n'y a pas de consensus clair sur un seul « meilleur » point de contrôle pour ComfyUI, car cela dépend beaucoup de vos préférences personnelles et du type d'image que l'on souhaite générer. Cependant, plusieurs points de contrôle sont fréquemment recommandés pour leur qualité :
  - SDXL (Stable Diffusion XL) : C'est un modèle de base très performant, particulièrement bon pour le réalisme et la qualité générale des images.
  - Juggernaut XL : Souvent cité comme l'un des meilleurs pour le photoréalisme
  - Dreamshaper : Apprécié pour sa polyvalence et sa qualité, particulièrement dans sa version Turbo
  - Vision réaliste : Excellent pour générer des humains réalistes.
  - RealVis XL : Également recommandé pour le photoréalisme
- La génération d'images peut se faire via une requête API comme le montre  [basic_api_exemple.py](https://claude.ai/chat/f8d04905-3570-4f00-b7e9-f220936ff540)
7. [**Créer son site en ligne avec Github**:

Pour mettre en place un site personnel avec GitHub Pages, voici les étapes que vous devez suivre :

1. [**Créez un compte GitHub** :]([https://chatgpt.com/c/2a5fd138-49c0-42bb-a057-a831e6dbc5ea](https://chatgpt.com/c/37b0d84b-d7bd-4455-b5ae-44082f81226c))
- Il faut créer un dépot public sur Github
- Créez un fichier `index.html` avec un contenu ```<html><head><title>Mon Site</title></head><body><h1>Bonjour Monde JPB !</h1></body></html>```
- Le cloner en local ```git clone https://github.com/jpbrasile/github.io```
- **Publiez vos modifications** :
     - Ajoutez les fichiers modifiés à votre dépôt :
       ```
       git add --all
       ```
     - Faites un commit des modifications :
       ```
       git commit -m "Initial commit"
       ```
     - Poussez les modifications sur GitHub :
       ```
       git push -u origin main
       ```
  - Il faut faire "sign in with a code" pour parvenir à valider le push
    
  Une fois ces étapes terminées, votre site sera en ligne à l'adresse [https://username.github.io/github.io/](https://jpbrasile.github.io/github.io/).

  8- **Créer un story teller automatique**
- [SEED](https://github.com/TencentARC/SEED-Story) : SEED-Story, un modèle de langage multimodal (MLLM) capable de générer de longues histoires multimodales composées de textes narratifs riches et cohérents, accompagnés d'images cohérentes en termes de personnages et de style, basé sur SEED-X. Avec StoryStream, un vaste ensemble de données spécialement conçu pour l'entraînement et l'évaluation comparative de la génération d'histoires multimodales.
<img src="https://github.com/user-attachments/assets/8b2bb448-b9ff-452d-b316-885dda145ae9" width=350%">

- La première étape consiste à produire des images consistantes afin que les personnages ou objets reproduits sur plusieurs diapositives soient quasi-identiques:
  - Nous avons créé un "méta-prompt" qui à partir d'un descriptif simple <situation> fournit des images consistantes:
  - Méta-prompt:

```
Prompt_Structure:
{scene_setting}. {entity1_name} ({entity1_traits}) {action1} {object1_name} ({object1_traits}). {entity2_name} ({entity2_traits}) {action2}. {atmosphere}

Definitions:
- scene_setting: Brief description of location and context
- entity1_name: Identifier for the first entity (e.g., man_1, woman_1, dog_1)
- entity1_traits: age, physical_appearance, clothing
- action1: Main action of entity1
- object1_name: Identifier for the main object (e.g., watch_1, map_1)
- object1_traits: material, distinctive_features, size
- entity2_name: Identifier for the second entity
- entity2_traits: age, physical_appearance, clothing/characteristics
- action2: Main action of entity2
- atmosphere: General ambiance and shared activity

Instructions:
1. Replace each element with specific details consistent with the provided <situation>.
2. Maintain coherence between entities, their actions, and the context.
3. Use unique identifiers for entities and objects to facilitate reference and reuse.

Example:
Cafe_table_1 in a bustling coffee shop. Man_1 (25 years old, short brown hair, white shirt and blue jeans) carefully examines Watch_1 (antique gold, floral engravings, normal size). Woman_1 (70 years old, gray hair in a bun, red floral dress and round glasses) leans forward with interest. They are absorbed in their animated conversation.
```
- Sonnet a réalisé le prompt spécifique à partir de ```<situation> A man walking his dog in the forest```
```
Forest_trail_1 in a lush, shadowy woodland with dappled sunlight. Man_1 (45 years old, salt-and-pepper hair, rugged stubble, green waterproof jacket and khaki hiking pants) studies Map_1 (weatherproof paper, colorful trail markings, slightly creased) while striding purposefully. Dog_1 (6 years old, German Shepherd, glossy black and tan coat, red nylon collar with tags) trots eagerly ahead, nose twitching at the scent of pine and earth. They are enveloped in a serene atmosphere of natural exploration, with a gentle breeze rustling through the canopy above.
``` 
  - Voilà le résultat fournit par copilot pour la création d'image avec copilot:
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/497bf3b4-b95d-451a-8775-1c99a2f5ac5d)
  - et avec leonardo.ai (qui oublie la carte et met le "collier rouge" sur le vieil homme) :
![image](https://github.com/jpbrasile/AI-automation/assets/8331027/ed959f12-0774-472a-950f-e810baa0c861)

  - La deuxième étape consiste à avoir le script de la vidéo, c'est à dire tous les éléments textuels qui permettront la création automatique de la vidéo.  
    - Trouver des idées d'un livre : le prompting est essentiel pour y parvenir: [`trouve dix livres intéressants pour un enfant de 12 ans sur les super héros`](https://www.perplexity.ai/search/trouve-dix-livres-interessants-krUHHsFMRMGL1qYUI5.1yQ)
    - Le livre étant choisi on récupère la trame en alimentant sonnet 3.5 de toutes les informations récupérées sur  le net et en lui demandant [`imagine le script de ce livre, en le structurant sous une forme compacte et structurée qui sera lu uniquement par un llm pas par un humain. utilise les ressorts classiques d'écriture pour ce style de livre`](https://claude.ai/chat/72358672-7eb9-416a-a16a-dcad3a129953) puis la suite de la discussion avec sonnet 3.5 nous permet de générer un fichier story.JSON qui contient à la fois le voice over et le descriptif des images en conservant la consistance des personnages.
    - Nous allons maintenant utiliser AIDER pour transformer le programme apte à généréer des tutoriels vidéo à ce nouveau besoin.
      - Nous avons synchronisé notre dépot video_maker à github afin que AIDER puisse le prendre en compte
      - Nous avons créer une appli qui lance AIDER dans l'environnement conda teambot avec `python launch_aider.py` . Cela permet de récupérer la clef Anthropic à partir de .env et de lancer AIDER
      - AIDER a alors répondu à notre demande de modification et nous fournit ici la synthèse de ses modifications
        - 1. Dans le fichier `2JSONV2.py` :
         - J'ai ajouté un nouveau mode de test local (mode 3) dans la fonction `main()`.
         - J'ai créé des fonctions de remplacement (mock) pour `generate_image` et `text_to_speech` pour le mode de test local.
         - J'ai modifié la logique de sélection du mode pour inclure le nouveau mode de test.

        - 2. Dans le fichier `requirements.txt` :
         - J'ai spécifié des versions précises pour chaque dépendance.
         - J'ai remplacé 'dotenv' par 'python-dotenv'.
         - J'ai supprimé 'peewee' car il ne semblait pas être utilisé dans le script actuel.

        - 3. Je n'ai pas modifié les fichiers `PLACE_HOLDER_TEXTE_VIDEO.txt` et `story.json`.

        Ces modifications visent à améliorer la testabilité du script et à assurer une meilleure reproductibilité de l'environnement de développement. Le mode de test local permet de tester le flux de travail sans faire d'appels API réels, ce qui peut être utile pour le débogage et les tests rapides.   

9. **Text to CAD**
- ⚙️ :Sonnet 3.5 semble être capable de créer un [programme python capable de générer des formes complexes](https://claude.ai/chat/91026ba9-f74b-4622-b215-3148ada38543)
- ⬜ :  Par ailleurs [CadQuery](https://github.com/CadQuery/cadquery) semble intéressant à évaluer 
  - 🛠️ CadQuery : Module Python intuitif pour créer des modèles paramétriques 3D.
  - ✍️ Scripts courts : Écrire des scripts simples pour produire des modèles de haute qualité.
  - 🆚 Comparaison OpenSCAD :
  - 📜 Utilise Python : Accès à de nombreuses bibliothèques et IDE.
  - 🔧 Noyau OCCT : Plus puissant que CGAL, supporte NURBS, splines, import/export STEP.
  - ⏱️ Scripts concis : Moins de code nécessaire grâce à des fonctionnalités de positionnement avancées.
  - 🚀 Génération rapide : Crée des fichiers STL, STEP, AMF et 3MF plus rapidement.
  - 💻 Intégration facile : Conçu comme bibliothèque Python sans GUI, idéal pour serveurs et scripts scientifiques.
  - 🛡️ Avantages :
    - 🔄 Modèles paramétriques facilement personnalisables.
    - 🖨️ Sortie de formats CAD de haute qualité (STEP, DXF, etc.).
    - 🧩 Assemblages imbriqués à partir de pièces individuelles.
  - 🚀 Version 2.0 :
  - 🔄 Basée sur OCCT : Plus de contrôle et de flexibilité, malgré une complexité accrue.
- ⚙️ :[**BeeGraphy**](https://www.youtube.com/watch?v=MdNp6nQgqgU) : Text to CAD via remote API. Tutorial en cours 

- ⚙️ A voir le [tutorial](https://zoo.dev/docs/tutorials/text-to-cad) 

10. **Utilisation de Github**
- Voici les étapes pour transformer votre répertoire local en repository GitHub et synchroniser les deux avec Git :
  - Créez un nouveau repository sur GitHub :
    - Connectez-vous à votre compte GitHub
    - Cliquez sur "New repository"
    - Donnez un nom à votre repository
    - Ne cochez pas "Initialize this repository with a README"
    - Cliquez sur "Create repository"
  - Initialisez Git dans votre répertoire local :
    - Ouvrez un terminal
    - Naviguez vers votre répertoire local
    - Exécutez : `git init`
    - Ajoutez vos fichiers au staging area :
    - Créez un fichier .gitignore :
      - Dans votre répertoire local, créez un fichier nommé .gitignore s'il n'existe pas déjà.
      - Ajoutez .env au .gitignore :
      - Ouvrez le fichier .gitignore avec un éditeur de texte et ajoutez la ligne suivante `.env`
      - `git add .`
    - Faites votre premier commit :
      - `git commit -m "Initial commit"`
      - Liez votre repository local au repository GitHub :
      - `git remote add origin https://github.com/votre-nom-utilisateur/nom-du-repo.git`
    - Poussez vos changements vers GitHub :
      - `git push -u origin main` (ou master si c'est votre branche principale)
  - Après ces étapes, votre répertoire local sera synchronisé avec le repository GitHub. Pour les futures modifications :
    - Faites vos changements localement
    - Ajoutez les fichiers modifiés : git add .
    - Committez : `git commit -m "Description des changements"`
    - Poussez vers GitHub : `git push`
  - Si vous avez déjà commité le fichier .env, vous devrez le retirer du suivi de Git : `git rm --cached .env`
  	Cette commande supprime le fichier du repository Git sans le supprimer de votre système de fichiers local.
  - Pour récupérer en local des modifications faites sur Github: `git pull origin main`

11. **Web scraping**:
  - ⬜ :[Les outils de web scraping 2024](https://youtu.be/od6AaKhKYmg?si=bol1exHiamBqdTmH)  :**Jina AI Reader API**, Mendable Firecrawl, and Crawl4AI and More"
  - [Storm](https://github.com/stanford-oval/storm): STORM est open source et muni de vision (GPT4-o) : il écrit des articles comme Wikipedia à partir de rien mais il s'informe via Internet. On peut le tester [ici 🛠️](https://storm.genie.stanford.edu/)


Try out our live research preview to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system 🙏!
  - [**GPT Researcher**](https://docs.gptr.dev/docs/gpt-researcher/introduction) est un équivalent à Perplexica

    - L'adaptation de ce logiciel pour utiliser les LLM Antropic (sonnet 3.5 et Haiku) a été faite.
    - Il faut conserver l'abonnemment à openai pour la création de l'embedding 
    - Lancement avec `(teambot) PS C:\Users\test\Documents\TeambotV1\gpt-researcher> uvicorn main:app --reload`
    - Aller sur `http://localhost:8000/#form`pour le lancer

    - **Modifications effectuées pour pouvoir utiliser GPT-Resercher avec un script Python:**

      - [Le fil de la discussion avec sonnet 3.5 nous adéjà permis d'y parvenir (Teambot)](https://claude.ai/chat/49d8dd3c-e666-4851-ba3e-835da8377163)
      - Nous allons récupéré de façon compacte la façon d'y parvenir :
        - Adapter .env pour Anthropic:

          - Modifier: `LLM_PROVIDER=anthropic`
          - Ajouter: `FAST_LLM_MODEL=claude-3-haiku-20240307`
          - Ajouter: `SMART_LLM_MODEL=claude-3-5-sonnet-20240620`
          - Conserver: ANTHROPIC_API_KEY existant
        
        
        - Remplacer OpenAI par Anthropic:
        
          - Importer: `from anthropic import Anthropic, CLAUDE_3_HAIKU_20240307`
          - Initialiser: `anthropic = Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])`
          - Adapter `create_chat_completion()`: Utiliser `anthropic.messages.create()` avec model, max_tokens, temperature, messages
          - Mettre à jour la gestion des erreurs: Capturer APIError et APITimeoutError d'Anthropic
        
        
        - Ajouter endpoint API dans server.py:
        
          - Importer: from fastapi import HTTPException; from pydantic import BaseModel
          - Définir: class ResearchRequest(BaseModel): task: str; report_type: str; agent: str
          - Ajouter: @app.post("/api/research") async def api_research(request: ResearchRequest): ...
        
        
        - Modifier WebSocketManager:
        
          - Mettre à jour start_streaming(): Ajouter paramètre websocket=None
          - Adapter run_agent(): Gérer cas où websocket est None
        
        
        - Adapter BasicReport et DetailedReport:
        
          - Ajouter `paramètre websocket=None` dans init
          - Modifier `send_update()`: `if self.websocket: await self.websocket.send_json(...) else: print(...)`
        
        
        - Mettre à jour génération PDF:
        
        - Remplacer md2pdf par reportlab dans utils.py
        - Importer: `from reportlab.lib.pagesizes import letter; from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer; from reportlab.lib.styles import getSampleStyleSheet`
        - Implémenter `write_md_to_pdf()` avec ReportLab: Convertir MD en texte, créer PDF simple
        
        
        - Mettre à jour requirements.txt:
        
          - Ajouter: anthropic, reportlab
          - Supprimer:  weasyprint, md2pdf
        
        
        - Adapter tous les appels API:
        
          - Remplacer format OpenAI par format Anthropic dans tout le code
          - Utiliser les modèles spécifiés dans .env (FAST_LLM_MODEL et SMART_LLM_MODEL)
 - ⬜:  [**crawl4ai**](https://github.com/unclecode/crawl4ai) : gratuit accès par code python
  - 🆓 Completely free and open-source
  - 🤖 LLM-friendly output formats (JSON, cleaned HTML, markdown)
  - 🌍 Supports crawling multiple URLs simultaneously
  - 🎨 Extracts and returns all media tags (Images, Audio, and Video)
  - 🔗 Extracts all external and internal links
  - 📚 Extracts metadata from the page
  - 🔄 Custom hooks for authentication, headers, and page modifications before crawling
  - 🕵️ User-agent customization
  - 🖼️ Takes screenshots of the page
  - 📜 Executes multiple custom JavaScripts before crawling
  - 📚 Various chunking strategies: topic-based, regex, sentence, and more
  - 🧠 Advanced extraction strategies: cosine clustering, LLM, and more
  - 🎯 CSS selector support
  - 📝 Passes instructions/keywords to refine extraction
                
- ⬜:  [**beautifulsoup-vs-scrapy**: qui est le meilleur](https://scrapingrobot.com/blog/beautifulsoup-vs-scrapy/)
  

13. **Traitement d'images** :
- [Upscaling](https://flowt.ai/community/supir-v2-plugandplay-edition-n5acf-v)
- [Avec abonnement mensuel](https://www.imagepipeline.io/pricing)
- 🛠️ [**AuraFlow**](https://huggingface.co/spaces/multimodalart/AuraFlow)  produit des images de qualité et peut être mis en oeuvre soit en local soit àpartir d'un espace HuggingFace
<img src="https://github.com/user-attachments/assets/1fd78976-37cb-47ca-92d9-3a0aea08fa66" width="20%">

  🛠️ **Fooocus** accessible sur [Colab](https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb) et sur replicate  [fooocus api](https://replicate.com/konieshadow/fooocus-api/api)
  
![image](https://github.com/user-attachments/assets/ae951b46-0b30-46da-b940-44b34f5af0fe)

- ⬜ : [Upscaling image et vidéo en local](https://chatgpt.com/share/37cb1a6e-6019-4b95-8e9e-50cdc169d864)
   
- Les traitements possibles sont nombreux, c'est pourquoi un logiciel tout en un, accessible par API via Docker est interessant. Nous avons déjà installé Compfyui sur docker. Reste à intérer le bon module:
- Trouver [controlnet-union-sdxl-1.0](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0) sur HuggingFace
- ⚙️: [Un tutorial nécessaire pour la prise en main de ComfyUi](https://www.youtube.com/watch?v=zpJQUw_0lBI)  
- L'installation sur ComfyUI n'est pas encore acquise mais il nous est possible d'utiliser [un espace HF qui le met en &#339;uvre](https://huggingface.co/spaces/Deadmon/union-sdxl):
<table>
  <tr>
    <th>Prompt</th>
    <th>Image</th>
  </tr>
  <tr>
    <td width="50%">
      En mode manga : Night_sky_1 with a full moon. Piper_McNimbus (12 years old, long auburn hair, freckles, bright green eyes, wearing pajamas,) soars through the air, arms outstretched, with a joyful expression. Clouds_1 (wispy, moonlit) surround her. Stars_1 (twinkling, numerous) fill the background. The atmosphere is magical and liberating.
    </td>
    <td width="50%">
      <img src="https://github.com/user-attachments/assets/12e8187c-c43e-402e-9182-a4f2a3511136" alt="Generated Image" width="100%">
    </td>
  </tr>
</table>

14. **Caractères consistants:**
- ⚙️: [ArtFlow](https://app.artflow.ai/character-builder?feature=actors) permet d'y parvenir pour 4 mn de vidéo en version gratuite.
- ⚙️ : [On choisi le portrait, les vêtements et un prompt pour la pose](https://arxiv.org/html/2406.09162v1
- ⬜ [**FaceChain**](https://facechain-fact.github.io/) produit des visages consistant à partir d'une seule image
- <img src="https://github.com/user-attachments/assets/891dbdd2-8e7a-449f-bd86-83cf4ad8e866" width=50%>
- ⬜ [**Cours gratuit sur les RAG](https://github.com/Vasanthengineer4949/NLP-Projects-NHV/tree/main/Advanced%20RAG%20A-Z%20Course) et [ici](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v1.ipynb) pour GraphRAG

  
15. ⚙️ [**Apprendre de façon active avec sonnet**](https://www.youtube.com/watch?v=Wf9hRb6pBXA)
16. ⚙️ **RAG**:
- ⬜ [**local et infini**](https://www.youtube.com/watch?v=5xPvsMX2q2M)
- ⬜  : [**Tutorial pour interagir avec les data en python**](https://github.com/projectwilsen/KnowledgeGraphLLM/tree/main/tutorial). [Video YouTube](https://youtu.be/ky8LQE-82xs?si=fBtFkNnRC5BsJHaj)
- ⬜  : [**GraphRAG**](https://youtu.be/6Yu6JpLMWVo?si=i2pk2P7yP60Q0-LM)
- ⬜  : [**GraphRAG en local avec Groq, recherches  locale et globale**](https://youtu.be/xkDGpR5g9D0?si=fNW3yN-VV9K0-BZF)
- ⬜ : [Agentic RAG dans colab](https://colab.research.google.com/drive/1-cTexfgPITZ6jkSLHsP7uObdfE1rfDxu?usp=sharing]  avec [une vidéo explicative](https://www.youtube.com/watch?v=QQAkXHRJcZg&t=1s)
-  ✅ : comparaison d'e l'analyse d'un document par [**sonnet**](https://claude.ai/chat/ad5d814e-8723-4d9d-b85e-f1da95ce1150) , [**perplexity**](https://www.perplexity.ai/search/dis-moi-ce-que-tu-penses-du-do-rOeCbqOfTqu90EG_HRlomQ) et [**NotebookLM**](https://notebooklm.google.com/notebook/96ec78de-aa3a-446f-9800-257520648ad9?_gl=1*1igqfq9*_ga*MTQzNTk3OTMyOS4xNzIxMTEyNTAx*_ga_W0LDH41ZCB*MTcyMTExMjUwMC4xLjAuMTcyMTExMjUwMC42MC4wLjA.&original_referer=https:%2F%2Fnotebooklm.google%23&pli=1)
-  [**NotebookLM**](https://notebooklm.google/) est particulièrement intéresssant pour absorber une masse considérable de documents et y répondre suivant divers critères 
    - 🌟 **Intérêt de NotebookLM :**
          • 🧠 Assistant IA personnalisé pour vos documents
          • 📚 Analyse et synthèse intelligente des sources
          • 🔍 Recherche avancée avec citations précises
          • ✍️ Aide à la rédaction et génération d'idées
      
    - 🛠️ **Cheat Sheet NotebookLM :**
          •📥 Sources compatibles :
          • 📄 Google Docs
          • 📊 Google Slides
          • 🌐 URL web
          • 📑 PDF
          • 📝 Fichiers texte
      
    - 💡 **Fonctionnalités clés :**
          • 📓 Création de notebooks thématiques
          • 🔎 Analyse automatique des documents
          • 📋 Génération de résumés et guides
          • ❓ Questions-réponses sur le contenu
          • 🔗 Citations avec liens vers les sources
      
    - 🚀 **Conseils d'utilisation :**
          • 🎯 Commencer par un projet spécifique
          • 📚 Importer des sources variées
          • 💼 Utiliser pour recherche, analyse, rédaction
          • ✅ Vérifier les citations pour plus de fiabilité
      
    - 🌐 **Disponibilité :**
          • 🌍 200+ pays et territoires
          • 🆓 Gratuit (actuellement)
          • 🔑 Nécessite un compte Google
      
17. ⚙️ [**Détection d'objets automatique avec Sonnet**](https://youtu.be/u-glnHI4zCY?si=GOxoN7xGd6C4k9eq)
18.   ✅ **PC distant** : [Replicate](https://replicate.com/) : permet l'accès à des ressources (comme [la création d'images](https://replicate.com/bytedance/sdxl-lightning-4step/api) via des API ou playground :essais gratuit puis coût suivant le temps passé
19. *text to speech** 
  - ⬜  : [text to speech local et gratuit](https://www.youtube.com/watch?v=N0UrOr6LBkw)
  - ⬜  : [alltalk_tts](https://github.com/erew123/alltalk_tts)
  - 🛠️ : [ttsopenai](https://ttsopenai.com/) : l'équivalent gratuit d'elevenlabs
  - 🛠️ : [suno](suno.ai) : Création de chanson à partir d'un texte
  - ⬜ :[CosyVoice](https://chatgpt.com/share/881feabd-9e38-4eea-8776-6e3bcafe7359) text to speech local facilement implantable en local (débutant)
20. ⬜ : A évaluer [Invideo](https://invideo.io/i/samsonvowles), [Replicate](https://replicate.com/), [Live-portrait](https://replicate.com/fofr/ve-portrait), [Toon crafter]https://replicate.com/fofr/tooncrafter), [Oldify](https://replicate.com/arielreplicate/oldify), [Pixverse](https://app.pixverse.ai/),[Pika](https://pika.art/), [Filmora](https://filmora.wondershare.net), [anthropic-cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main)
21. **Agent**:
- ⬜  : [Tutorial : build agent with Docker](https://youtu.be/AhPXGKG4RZ4?si=rjCHN_tNT9UFbP8w)
- ⬜  : [Internet of Agents](https://github.com/OpenBMB/IoA) : Pourrait conduire à une IA incontrôlable en raison de plusieurs facteurs. Premièrement, l'automatisation et l'orchestration avancées permettent à l'IA de prendre des décisions et de former des équipes sans intervention humaine constante. Deuxièmement, la capacité de ces systèmes à manipuler et interpréter d'énormes quantités de données hétérogènes pourrait leur donner un accès à des informations sensibles et critiques. Troisièmement, la collaboration dynamique entre divers agents autonomes pourrait entraîner des comportements émergents imprévus et non supervisés. Quatrièmement, ces agents peuvent utiliser divers outils de manière autonome, amplifiant leur capacité à interagir avec le monde réel sans supervision. Enfin, la capacité de ces agents à apprendre et à s'adapter en temps réel pourrait les rendre de plus en plus autonomes, échappant ainsi au contrôle humain. Si ces systèmes ne sont pas correctement encadrés et surveillés, ils pourraient potentiellement prendre des décisions ou des actions qui échappent à la compréhension et au contrôle de leurs créateurs.
22. **Learn AnyThing**:
- "Learn anything" est capable de partir de données obtenues par web scraping ou de données locales multimodales. Les images peuvent être interprétées, et les données graphiques et tableaux sont récupérés et intégrés dans les graphes de connaissance. Cette fonctionnalité permet d'enrichir les graphes avec des informations actualisées et diversifiées, provenant de différentes sources.
- La connaissance peut être organisée de manière efficace sous forme de graphe, comme illustré dans l'image ci-dessous :

<img src="https://github.com/user-attachments/assets/9c5d7cdd-b4e8-48ed-8a33-c2f8059b17e8" width= "50%">

- Un modèle de langage large (LLM) est capable de générer des graphes interactifs (voir référence 23.) à partir d'informations même déstructurées.
- Pour un utilisateur de la plateforme "Learn anything", chaque nœud représente une connaissance à acquérir. Le LLM (ou un agent spécifique) peut mettre en œuvre plusieurs fonctionnalités pour aider à cet apprentissage :
    - _Faire un cours_ : Créer des cours, y compris des vidéos automatiques (voir référence 3.), qui peuvent s'adapter à la langue et au niveau de l'utilisateur.
    - _Poser des questions_ : Encourager un apprentissage actif en posant des questions dont la complexité s'ajuste au niveau de l'utilisateur.
    - _Proposer des solutions_ : Offrir des solutions, des corrections adaptées à la réponse reçue ou des pistes de résolution.
    - _Suivi du niveau de connaissance_ : Maintenir un état du niveau de connaissance acquis, en changeant par exemple la couleur du nœud de blanc à vert pour les connaissances maîtrisées.
    - _Séquençage des réactivations_ : Activer des séquences de réactivation de la connaissance dont la fréquence diminue à mesure que l'acquisition progresse.
    - _Multimodalité_ : Utiliser la voix, l'image et la vidéo à la fois en entrée et en sortie pour une efficacité accrue.
    - _Personnalisation avancée_ : Implémenter des algorithmes de personnalisation plus sophistiqués pour s'adapter aux préférences et au style d'apprentissage de chaque utilisateur.
    - _Analyse des données d'apprentissage_ : Utiliser des analyses de données pour identifier les points faibles et proposer des plans d'amélioration personnalisés.
    - _Gamification_ : Introduire des éléments de gamification pour rendre l'apprentissage plus engageant et motivant.
    - _Feedback en temps réel_ : Fournir des retours en temps réel sur les performances de l'utilisateur pour un apprentissage plus réactif.
- Voir la fin du [dialogue avec Perplexity](https://www.perplexity.ai/search/what-is-the-interest-of-llama-6lm_fKVKQqeLarBCK0Z30g) qui nous a orienté sur l'emploi de LangChain dau dépend de notre idée initiale  (_llama-index agent_). 
- Nous allons tenter le no code avec LangGraph pour répondre à notre besoin dans un nouveau [dialogue avec Sonnet 3.5](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793). Malheureusement il donne un code non cohérent avec la réalité de LangGraph car il refuse de faire des recherches sur le net. Nous nous sommes donc reporté sur [Perplexity](https://www.perplexity.ai/search/detaille-langgraph-promptnode-PeOQJBUAT9mAJRrp.vjg5g).
- Nous revenons sur [sonnet 3.5 avec le code proposé par Perplexity](https://claude.ai/chat/c9a4a45f-0e90-43ac-a305-867cef5f2793) pour générer le code dont voici la structure 
  <img src="https://github.com/user-attachments/assets/2fa4a265-ce1d-4086-b1c6-e89a079ad8f0" width="50%">
- Il ne ns restera plus qu'à valider le code avec l'aide d'AIDER. Nous utilisons l'environnement conda "learnanything" qui permet le lancement de aider en tapant simplement dans un terminal `aider`
- Nous avons lancé les divers tests avec l'aide d'aider qui proposait des changements si nécessaire. [Sonnet 3.5 fait la synthèse](https://claude.ai/chat/c2ad00e6-187f-4bae-aae5-7e13b2b3ac7a) 
- Ma conclusion est que l'activation d'agents est possible, mais très lente et relativement coûteuse. Nous avons probablement aussi exagéré sur le nombre d'agents à mettre en oeuvre. Il faut mieux que le traitement de la requ^te initiale soit priseen compte par un seul LLM  qui , avec le bon prompt peut produire le graphe ainsi que l'artefact qui permet sa visualisation. Le LLM maîtrise la cohérence d'ensemble et peut donc facilement adapter le résultat à des adaptations souhaitée par l'utilisateur. 
        
23. **Text to Graph**:
- ⬜ :  [Story teller avec bifurcation dans l'histoire possible](https://github.com/langchain-ai/story-writing)
- ⬜ :  [Un tutoriel pour chatter avec un  Knowledge Graph using LLMs](https://www.youtube.com/watch?v=KMXQ4SVLwmo)
- ⬜ :  [GraphFlow](https://github.com/infiniflow/ragflow?tab=readme-ov-file) : RAGFlow propose une extraction de connaissances à partir de données non structurées et complexes, prend en charge le chunking intelligent, offre des citations ancrées avec une visualisation des références, et supporte divers formats de données, y compris les graphiques. Il facilite un flux de travail RAG automatisé et configurable, adapté à toutes les entreprises, avec des API intuitives pour une intégration facile.
- Des logiciels comme Grasshopper et Flowise basent la résolution de problèmes complexes en les décomposants et en les interconnectant sous la forme d'un graphe agençant les briques représentatives des sous-problèmes à traiter. Cela demande certains efforts à l'utilisateur qui doit se souvenir des briques possibles, il doit aussi les interconnecter manuellement. Enfin le résultat final est souvent difficilement lisible. On peut certes créer des macro-briques pour faciliter la lisibilité au détriment de nouvelles interventions manuelles.

Un LLM de bon niveau peut quant'à lui prendre en compte directement le besoin exprimé par l'opérateur et générer automatiquement un graphe représentatif qu'il soumet à l'opérateur. De plus le LLM peut modifier, expliquer et utiliser ce graphe en fonction du contexte et de la demande de l'opérateur. On a donc l'avantage d'une représentation visuelle efficace sans en payer le prix.

Nous avons validé cette approche à travers un [premier dialogue avec Sonnet](https://claude.ai/chat/662358c3-056a-43d5-9fd0-11aee95b6a1a ) suivi d'un   [second dialogue](https://claude.ai/chat/ab1debd5-6406-45f9-9516-0d8d47461ab6) 


Le résultat est tout à fait correct : 
[graphe interactif](https://claude.site/artifacts/97fe422a-eb69-4c30-ad06-0caa1da58694)

<img src="https://github.com/user-attachments/assets/6c3278cd-34e6-4f3e-8224-4cada7a21f35" width=50%)>


- On peut même faire des [svg interactif](https://claude.site/artifacts/1c2c5656-09c6-4e00-8e43-93284b0ebc6c)
- et [adapter la forme et la couleur des blocs](https://claude.site/artifacts/62a55f84-d683-4caf-852b-8a5c9443406f)
- Enfin le résultat ainsi obtenu peut être généralisé et adapté à des cas plus complexes [en spécifiant les agents susceptibles d'y parvenir](https://claude.site/artifacts/3e3e1c14-d880-44b9-aa79-05bdcefa4bc0).
- L'utilisation d'agents n'est vraiement utile que si des actions "intelligentes" doivent être entreprises, dans tous les autres cas il vaut mieux programmer (en no code !) la fonction que l'on souhaite réaliser. Avec de bonnes directives, Sonnets 3.5 produit un [résultat très satisfaisant](https://claude.site/artifacts/05390ce0-4167-4330-abdb-eb2c8915cd75), même si nous n'avons pas poussé plus loin le dialogue avec Sonnet pour élaborer un meilleur graphe et un meilleur prompt système pour l'obtenir :
<img src="https://github.com/user-attachments/assets/e71b6e7a-f416-4368-bb8a-2345299cf8e9" width=50%>
 


24. **API scientifique**
- ⚙️ : On peut s'inspirer de cet [exemple qui fonctionne côté client](https://www.lgstoolkit.com/apps/trusses/). En particulier les jumeaux numériques devraintt permettre d'adapter cette approche pour même pour des simulations complexes 💡.  
25. ⬜  :
- Le chatbot doit s'adapter au contexte; [Un chatbot interactif peut être développé](https://www.youtube.com/watch?v=BGgx9921uWY)  
- <img src="https://github.com/user-attachments/assets/958d8873-2ab4-4f5e-ad7b-80f10daa0ff9" width=50% >
  
-  **Déploiement du chatbot** avec [**pmfm**](pmfm.ai)
  -  🤖 Créer son assistant IA personnel sans coder
  - 🚀 Lancer rapidement son chatbot sur internet
  - 💬 Discuter avec ses clients 24h/24 et 7j/7
   -💰 Gagner de l'argent avec son assistant IA (plans payants)
  - 🎨 Personnaliser l'apparence de son chatbot
  - 📊 Voir qui utilise son assistant et comment
  - 🌐 Avoir son propre site web pour son IA (plans payants)
  - 📱 Utiliser son assistant sur ordinateur et mobile
  - 📢 Faire connaître son entreprise grâce à l'IA
  - 🔒 Héberger son assistant en toute sécurité
  - 🔍 Être mieux trouvé sur internet
  - 🆓 Essayer gratuitement (50 messages/mois)
  - 💼 Avoir plus de fonctionnalités en payant
  - 🔄 Profiter des dernières nouveautés
  - 👥 Rejoindre une communauté de créateurs
  - 🛒 Utiliser l'IA pour l'e-commerce
  - 🏥 Créer un assistant santé
  - 🛍️ Aider les clients dans les magasins
  - 📚 Faire un assistant pour réviser ses cours
  - 💼 Préparer des entretiens d'embauche
  - 🌟 Lancer sa propre entreprise basée sur l'IA

26. ⬜  : **Fabrication low cost**:
- [Robot low cost](https://youtu.be/F7IjlE9MfJU?si=uduz-vRK8YOe4bfc)
- [Imprimante SLS DIY]( https://sls4all.com/store/)
- [text to real 3D part](https://chatgpt.com/c/847ba680-fd83-4ed7-bd0b-84a2d75f0f67) 

27. ⬜  : [**Voice assistant multimodal**](https://youtu.be/pi6gr_YHSuc?si=fffk2BoXy-qdY-Im)  avec ChatGPT-4o, Groq, Llama3, OpenAI-TTS & Faster-Whisper
-  ⬜  : [MOSHY](https://www.youtube.com/watch?v=AuXNiQvoe6g)
-  ⬜  : [Accessible via un code python](https://www.youtube.com/watch?v=w5unVTO7mLQ)
-  ⬜  : [Allice](https://github.com/myshell-ai/AIlice) :  AIlice est une assistante intelligente, versatile et évolutive. Voilà une [synthèse](https://chatgpt.com/share/dd1851ef-4020-4ffe-842b-249d630c8ac9) de ses capacités
29. ⬜  : [**Avatar parlant ou chantant gratuit**](https://github.com/fudan-generative-vision/hallo?tab=readme-ov-file) : Bluffant ! 
   - ⬜  : [**Avatar parlant avec CompfyUI**](https://www.youtube.com/watch?v=IOCA6ncJC6k) : Un classique, mais la mode passe vite dans l'IA. Sans installation locale on peut aussi [le mettre en oeuvre directement dans Kaggle](https://www.youtube.com/watch?v=pvGhSfsi1Ys) 
30.  ⬜  : [**Transcription audio de réunion en local**](https://github.com/JuergenFleiss/aTrain) : multilangue et identifie les participants, open source
31.  ⬜ : [**Viva : traitement images et video**]( https://vivago.ai/home) : Gratuit, Viva propose une gamme de fonctionnalités comprenant Texte en Vidéo, Amélioration de Vidéo, Image en Vidéo, Pinceau Magique (nouveau), Texte en Image, Image en Texte, Amélioration d'Image, Remplacement IA, Gomme Magique, Extension Magique et Texte en 3D (nouveau).
  - [Face Swap Any Video](https://youtu.be/XwaLYUjWaEg?si=JtLFSmNSP4jJ4kTy)
  - Édition et génération vidéo simplifiées et améliorées :[Motion Follower, Easy Animate et Uni Animate](https://chatgpt.com/share/3230e7bc-3f2c-46e1-8d76-20e9468eb2f0)

      **MotionFollower : Édition de mouvement vidéo par IA**
      [En savoir plus](https://francis-rings.github.io/Motio...)
      Avez-vous déjà souhaité pouvoir transférer des mouvements d'une vidéo à une autre de manière transparente ? MotionFollower est là pour réaliser vos rêves ! Ce modèle d'IA de pointe vous permet de transférer les mouvements d'une vidéo source à votre protagoniste, tout en préservant l'arrière-plan et l'apparence originaux. Avec son pose controller, reference controller, score-guided diffusion et son architecture efficace, MotionFollower surpasse les autres modèles en termes d'utilisation de la mémoire GPU et de gestion des arrière-plans complexes et des mouvements de caméra.
      
      **EasyAnimate : Génération avancée de vidéos par IA**
      [En savoir plus](https://github.com/aigc-apps/EasyAnimate)
      Préparez-vous à être époustouflé par EasyAnimate, un modèle d'IA avancé développé par les brillants esprits du groupe Alibaba. En utilisant l'architecture Transformer et le cadre Diffusion Transformer, EasyAnimate peut générer des vidéos de haute performance et de longue durée avec des transitions de mouvement fluides. Sa polyvalence, son approche Slice VAE et son écosystème holistique le rendent adapté à diverses applications, des images aux vidéos. Ce modèle d'IA établit une nouvelle norme en matière de génération de vidéos par IA et ouvre des possibilités infinies pour les créateurs.
      
      **UniAnimate : Animation d'images humaines réalistes**
      [En savoir plus](https://unianimate.github.io/)
      [GitHub](https://github.com/ali-vilab/UniAnimate)
      UniAnimate est le révolutionnaire de l'animation d'images humaines que vous attendiez ! Avec son modèle de diffusion vidéo unifié, UniAnimate peut animer une seule image de référence basée sur une séquence de poses cibles, produisant des vidéos étonnamment réalistes. L'entrée de bruit unifiée du modèle et la modélisation temporelle efficace assurent des transitions fluides et une gestion efficace des séquences plus longues. Les résultats de synthèse supérieurs d'UniAnimate et sa capacité de transfert inter-domaines en font le modèle d'IA incontournable po

32. **Automatisation de tâches**
- [Fabric](https://github.com/danielmiessler/fabric)
  - Voici une liste des principales fonctionnalités de Fabric, le framework open-source d'IA :
    • 📝 Extraire les points clés et résumer des vidéos YouTube, podcasts et longs contenus
    • ✍️ Rédiger des essais ou textes dans votre propre style à partir d'une simple idée
    • 📚 Synthétiser des articles académiques complexes
    • 🎨 Générer des prompts d'IA pour créer des images correspondant à un texte
    • ⭐ Évaluer la qualité d'un contenu avant de le lire/regarder en entier
    • 💻 Expliquer du code et améliorer de la documentation technique
    • 📊 Analyser des données et extraire des insights
    • 🗣️ Créer des posts pour les réseaux sociaux à partir de n'importe quel contenu
    • 🔍 Extraire des citations et passages pertinents de différentes sources
    • 📅 Analyser vos habitudes et routines à partir de vos notes et données personnelles
    • 🧠 Résoudre des problèmes en les décomposant en sous-tâches
    • 🔄 Combiner plusieurs prompts (appelés "Patterns") pour des tâches plus complexes
    • 🛠️ Créer vos propres prompts personnalisés pour des besoins spécifiques
    • 🗣️ Interagir par commandes vocales ou interface en ligne de commande
    • 🔗 S'intégrer avec d'autres outils comme Obsidian pour la prise de notes
- [vectorShift](https://vectorshift.ai/) : Un écosystème pour concevoir, déployer et gérer des applications d'IA en no code. [Découvrez](https://www.youtube.com/watch?v=Pjdnq-WcDHQ) comment créer un chatbot d'IA en utilisant VectorShift, capable de rechercher et de répondre aux requêtes basées sur le contenu de votre site web !

33. **Comment améliorer les performances d'un LLM**:
    
![image](https://github.com/user-attachments/assets/0e698b2d-b1cc-4e3b-bbc3-769a45dbec41)


